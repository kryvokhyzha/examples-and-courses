{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad755e7f-097a-47ef-af96-db14f4c617e5",
   "metadata": {},
   "source": [
    "This notebook is based on [DLA Seminar](https://github.com/markovka17/dla/blob/2022/week06/seminar.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac684a2-d865-446f-84df-212ccb25604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, scheduler, device, epoch):\n",
    "    model.train()\n",
    "\n",
    "    avg_loss = 0\n",
    "    step = epoch * len(dataloader)\n",
    "    for batch_idx, (wav, label) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        wav, label = wav.to(device), label.to(device)\n",
    "\n",
    "        preds = model(wav)\n",
    "        loss = criterion(preds, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        \n",
    "        wandb.log({\n",
    "            \"train_step_loss\": loss.item(),\n",
    "            \"lr\": scheduler.get_last_lr()[0], # get current lr for the 0th param group\n",
    "            \"acc_step\": count_acc(preds, label),\n",
    "            \"fa_step\": count_fa(preds, label),\n",
    "            \"fr_step\": count_fr(preds, label),\n",
    "        }, step=step + batch_idx)\n",
    "\n",
    "        if batch_idx == 0:        \n",
    "            wandb.log({\"train_image\": wandb.Audio(wav[0].detach().cpu().numpy(), sample_rate=16000,\n",
    "                                                  caption=f\"Label: {label[0]}, Pred: {preds[0].argmax(-1)}\")},\n",
    "                      step=step+batch_idx)\n",
    "\n",
    "    avg_loss = avg_loss / (batch_idx + 1)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    avg_loss = 0\n",
    "    accuracy = 0\n",
    "    fa = 0\n",
    "    fr = 0\n",
    "    total_elements = 0\n",
    "    for batch_idx, (wav, label) in enumerate(dataloader):\n",
    "        wav, label = wav.to(device), label.to(device)\n",
    "\n",
    "        preds = model(wav)\n",
    "        loss = criterion(preds, label)\n",
    "\n",
    "        accuracy += count_acc(preds, label)\n",
    "        fa += count_fa(preds, label)\n",
    "        fr += count_fr(preds, label)\n",
    "        avg_loss += loss.item()\n",
    "        \n",
    "\n",
    "    avg_loss = avg_loss / (batch_idx + 1)\n",
    "    accuracy = accuracy / (batch_idx + 1)\n",
    "    fa = fa / (batch_idx + 1)\n",
    "    fr = fr / (batch_idx + 1)\n",
    "\n",
    "    return avg_loss, accuracy, fa, fr\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, device, n_epochs):\n",
    "\n",
    "    train_avg_losses = []\n",
    "    val_avg_losses = []\n",
    "    val_accuracy_list = []\n",
    "\n",
    "    for epoch in range(n_epochs):        \n",
    "        train_avg_loss = train_one_epoch(model, train_dataloader, criterion, optimizer, scheduler, device, epoch)\n",
    "        val_avg_loss, val_accuracy, val_fa, val_fr = evaluate(model, val_dataloader, criterion, device)\n",
    "\n",
    "        wandb.log({\n",
    "            \"train_avg_loss\": train_avg_loss, \n",
    "            \"val_avg_loss\": val_avg_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_fa\": val_fa,\n",
    "            \"val_fr\": val_fr,\n",
    "        }, step=(epoch + 1) * len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09c197-776c-433f-9275-9561f2000d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import torch\n",
    "from typing import Tuple, Union, List, Callable, Optional\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class TaskConfig:\n",
    "    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n",
    "    batch_size: int = 128\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 1e-5\n",
    "    num_epochs: int = 20\n",
    "    n_mels: int = 40\n",
    "    cnn_out_channels: int = 8\n",
    "    kernel_size: Tuple[int, int] = (5, 20)\n",
    "    stride: Tuple[int, int] = (2, 8)\n",
    "    hidden_size: int = 64\n",
    "    gru_num_layers: int = 2\n",
    "    bidirectional: bool = False\n",
    "    num_classes: int = 2\n",
    "    sample_rate: int = 16000\n",
    "    device: torch.device = torch.device(\n",
    "        'cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4b4bf-3687-4885-a826-0beaae4b0146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -O speech_commands_v0.01.tar.gz\n",
    "# !mkdir speech_commands && tar -C speech_commands -xvzf speech_commands_v0.01.tar.gz 1> log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dee0e1-4b94-44e1-ab94-a471ec5d028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3125256-a635-4a7f-86dc-175bedcdd3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "def read_json(fname):\n",
    "    fname = Path(fname)\n",
    "    with fname.open(\"rt\") as handle:\n",
    "        return json.load(handle, object_hook=OrderedDict)\n",
    "\n",
    "\n",
    "def write_json(content, fname):\n",
    "    fname = Path(fname)\n",
    "    with fname.open(\"wt\") as handle:\n",
    "        json.dump(content, handle, indent=4, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b0a0b-40b9-44e4-b784-df2a8443cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommandDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform: Optional[Callable] = None,\n",
    "        path2dir: str = None,\n",
    "        keywords: Union[str, List[str]] = None,\n",
    "        csv: Optional[pd.DataFrame] = None,\n",
    "        part: \"str\" = \"train\",\n",
    "    ):        \n",
    "        self.transform = transform\n",
    "\n",
    "        self.path2dir = path2dir\n",
    "        self.keywords = keywords\n",
    "        self.index = self.create_or_load_index(part)\n",
    "\n",
    "    def create_or_load_index(self, part):\n",
    "        index_path = Path(f\"{part}_index.json\")\n",
    "        \n",
    "        if not index_path.exists():\n",
    "            self.create_index(part)\n",
    "            \n",
    "        return read_json(index_path)\n",
    "\n",
    "    def create_index(self, part):\n",
    "        path2dir = Path(self.path2dir)\n",
    "        keywords = self.keywords if isinstance(self.keywords, list) else [self.keywords]\n",
    "        \n",
    "        all_keywords = [\n",
    "            p.stem for p in path2dir.glob('*')\n",
    "            if p.is_dir() and not p.stem.startswith('_')\n",
    "        ]\n",
    "\n",
    "        index = []\n",
    "        for keyword in all_keywords:\n",
    "            paths = (path2dir / keyword).rglob('*.wav')\n",
    "            if keyword in keywords:\n",
    "                for path2wav in paths:\n",
    "                    index.append({\n",
    "                        \"path\": path2wav.as_posix(),\n",
    "                        \"keyword\": keyword,\n",
    "                        \"label\": 1\n",
    "                    })\n",
    "            else:\n",
    "                for path2wav in paths:\n",
    "                    index.append({\n",
    "                        \"path\": path2wav.as_posix(),\n",
    "                        \"keyword\": keyword,\n",
    "                        \"label\": 0\n",
    "                    })\n",
    "\n",
    "        torch.manual_seed(0)\n",
    "        indexes = torch.randperm(len(index))\n",
    "        train_indexes = indexes[:int(len(index) * 0.8)]\n",
    "        val_indexes = indexes[int(len(index) * 0.8):]\n",
    "\n",
    "        train_index = [index[i] for i in train_indexes]\n",
    "        val_index = [index[i] for i in val_indexes]\n",
    "\n",
    "        train_index_path = pathlib.Path(\"train_index.json\")\n",
    "        write_json(train_index, str(train_index_path))\n",
    "        \n",
    "        val_index_path = pathlib.Path(\"val_index.json\")\n",
    "        write_json(val_index, str(val_index_path))\n",
    "\n",
    "    def __getitem__(self, ind: int):\n",
    "        instance = self.index[ind]\n",
    "\n",
    "        path2wav = instance['path']\n",
    "        wav, sr = torchaudio.load(path2wav)\n",
    "        wav = wav.sum(dim=0)\n",
    "        \n",
    "        if self.transform:\n",
    "            wav = self.transform(wav)\n",
    "\n",
    "        return {\n",
    "            'wav': wav,\n",
    "            'keywors': instance['keyword'],\n",
    "            'label': instance['label']\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4dfa68-7138-4f97-b482-b43e02f40f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNNV3(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spec = LogMelspec(config)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.mel_spec(input)\n",
    "        \n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNNV3(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spec = LogMelspec(config)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.mel_spec(input)\n",
    "        \n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNNV3(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spec = LogMelspec(config)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.mel_spec(input)\n",
    "        \n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNNV3(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spec = LogMelspec(config)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.mel_spec(input)\n",
    "        \n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNNV3(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spec = LogMelspec(config)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.mel_spec(input)\n",
    "        \n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return outputclass CRNNV3(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spec = LogMelspec(config)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.mel_spec(input)\n",
    "        \n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNNV3(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spec = LogMelspec(config)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.mel_spec(input)\n",
    "        \n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNNV3(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spec = LogMelspec(config)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.mel_spec(input)\n",
    "        \n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNNV3(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spec = LogMelspec(config)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.mel_spec(input)\n",
    "        \n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNNV3(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spec = LogMelspec(config)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.mel_spec(input)\n",
    "        \n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNNV3(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spec = LogMelspec(config)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.mel_spec(input)\n",
    "        \n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNNV3(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spec = LogMelspec(config)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.mel_spec(input)\n",
    "        \n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNNV3(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spec = LogMelspec(config)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.mel_spec(input)\n",
    "        \n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNNV3(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spec = LogMelspec(config)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.mel_spec(input)\n",
    "        \n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e704499-9c1f-40a0-a3ad-5cde8cdd25f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommandDatasetV3(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform: Optional[Callable] = None,\n",
    "        path2dir: str = None,\n",
    "        keywords: Union[str, List[str]] = None,\n",
    "        csv: Optional[pd.DataFrame] = None,\n",
    "        part: \"str\" = \"train\",\n",
    "    ):        \n",
    "        self.transform = transform\n",
    "\n",
    "        self.path2dir = path2dir\n",
    "        self.keywords = keywords\n",
    "        self.index = self.create_or_load_index(part)\n",
    "\n",
    "    def create_or_load_index(self, part):\n",
    "        index_path = Path(f\"{part}_index.json\")\n",
    "        \n",
    "        if not index_path.exists():\n",
    "            self.create_index(part)\n",
    "            \n",
    "        return read_json(index_path)\n",
    "\n",
    "    def create_index(self, part):\n",
    "        path2dir = Path(self.path2dir)\n",
    "        keywords = self.keywords if isinstance(self.keywords, list) else [self.keywords]\n",
    "        \n",
    "        all_keywords = [\n",
    "            p.stem for p in path2dir.glob('*')\n",
    "            if p.is_dir() and not p.stem.startswith('_')\n",
    "        ]\n",
    "\n",
    "        index = []\n",
    "        for keyword in all_keywords:\n",
    "            paths = (path2dir / keyword).rglob('*.wav')\n",
    "            if keyword in keywords:\n",
    "                for path2wav in paths:\n",
    "                    index.append({\n",
    "                        \"path\": path2wav.as_posix(),\n",
    "                        \"keyword\": keyword,\n",
    "                        \"label\": 1\n",
    "                    })\n",
    "            else:\n",
    "                for path2wav in paths:\n",
    "                    index.append({\n",
    "                        \"path\": path2wav.as_posix(),\n",
    "                        \"keyword\": keyword,\n",
    "                        \"label\": 0\n",
    "                    })\n",
    "\n",
    "        torch.manual_seed(0)\n",
    "        indexes = torch.randperm(len(index))\n",
    "        train_indexes = indexes[:int(len(index) * 0.8)]\n",
    "        val_indexes = indexes[int(len(index) * 0.8):]\n",
    "\n",
    "        train_index = [index[i] for i in train_indexes]\n",
    "        val_index = [index[i] for i in val_indexes]\n",
    "\n",
    "        train_index_path = pathlib.Path(\"train_index.json\")\n",
    "        write_json(train_index, str(train_index_path))\n",
    "        \n",
    "        val_index_path = pathlib.Path(\"val_index.json\")\n",
    "        write_json(val_index, str(val_index_path))\n",
    "\n",
    "    def __getitem__(self, ind: int):\n",
    "        instance = self.index[ind]\n",
    "\n",
    "        path2wav = instance['path']\n",
    "        wav, sr = torchaudio.load(path2wav)\n",
    "        wav = wav.sum(dim=0)\n",
    "        \n",
    "        if self.transform:\n",
    "            wav = self.transform(wav)\n",
    "\n",
    "        return {\n",
    "            'wav': wav,\n",
    "            'keywors': instance['keyword'],\n",
    "            'label': instance['label']\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5caeba-a61e-4c9e-b263-b0eb81c0bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommandDatasetV4(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform: Optional[Callable] = None,\n",
    "        path2dir: str = None,\n",
    "        keywords: Union[str, List[str]] = None,\n",
    "        csv: Optional[pd.DataFrame] = None,\n",
    "        part: \"str\" = \"train\",\n",
    "    ):        \n",
    "        self.transform = transform\n",
    "\n",
    "        self.path2dir = path2dir\n",
    "        self.keywords = keywords\n",
    "        self.index = self.create_or_load_index(part)\n",
    "\n",
    "    def create_or_load_index(self, part):\n",
    "        index_path = Path(f\"{part}_index.json\")\n",
    "        \n",
    "        if not index_path.exists():\n",
    "            self.create_index(part)\n",
    "            \n",
    "        return read_json(index_path)\n",
    "\n",
    "    def create_index(self, part):\n",
    "        path2dir = Path(self.path2dir)\n",
    "        keywords = self.keywords if isinstance(self.keywords, list) else [self.keywords]\n",
    "        \n",
    "        all_keywords = [\n",
    "            p.stem for p in path2dir.glob('*')\n",
    "            if p.is_dir() and not p.stem.startswith('_')\n",
    "        ]\n",
    "\n",
    "        index = []\n",
    "        for keyword in all_keywords:\n",
    "            paths = (path2dir / keyword).rglob('*.wav')\n",
    "            if keyword in keywords:\n",
    "                for path2wav in paths:\n",
    "                    index.append({\n",
    "                        \"path\": path2wav.as_posix(),\n",
    "                        \"keyword\": keyword,\n",
    "                        \"label\": 1\n",
    "                    })\n",
    "            else:\n",
    "                for path2wav in paths:\n",
    "                    index.append({\n",
    "                        \"path\": path2wav.as_posix(),\n",
    "                        \"keyword\": keyword,\n",
    "                        \"label\": 0\n",
    "                    })\n",
    "\n",
    "        torch.manual_seed(0)\n",
    "        indexes = torch.randperm(len(index))\n",
    "        train_indexes = indexes[:int(len(index) * 0.8)]\n",
    "        val_indexes = indexes[int(len(index) * 0.8):]\n",
    "\n",
    "        train_index = [index[i] for i in train_indexes]\n",
    "        val_index = [index[i] for i in val_indexes]\n",
    "\n",
    "        train_index_path = pathlib.Path(\"train_index.json\")\n",
    "        write_json(train_index, str(train_index_path))\n",
    "        \n",
    "        val_index_path = pathlib.Path(\"val_index.json\")\n",
    "        write_json(val_index, str(val_index_path))\n",
    "\n",
    "    def __getitem__(self, ind: int):\n",
    "        instance = self.index[ind]\n",
    "\n",
    "        path2wav = instance['path']\n",
    "        wav, sr = torchaudio.load(path2wav)\n",
    "        wav = wav.sum(dim=0)\n",
    "        \n",
    "        if self.transform:\n",
    "            wav = self.transform(wav)\n",
    "\n",
    "        return {\n",
    "            'wav': wav,\n",
    "            'keywors': instance['keyword'],\n",
    "            'label': instance['label']\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3861ca2-70de-4ed3-9cdd-82d37ad4cc13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b777e5-5a57-4251-90e5-f291a540a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "class AugsCreation:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.background_noises = [\n",
    "            'speech_commands/_background_noise_/white_noise.wav',\n",
    "            'speech_commands/_background_noise_/dude_miaowing.wav',\n",
    "            'speech_commands/_background_noise_/doing_the_dishes.wav',\n",
    "            'speech_commands/_background_noise_/exercise_bike.wav',\n",
    "            'speech_commands/_background_noise_/pink_noise.wav',\n",
    "            'speech_commands/_background_noise_/running_tap.wav'\n",
    "        ]\n",
    "\n",
    "        self.noises = [\n",
    "            torchaudio.load(p)[0].squeeze()\n",
    "            for p in self.background_noises\n",
    "        ]\n",
    "\n",
    "    def add_rand_noise(self, audio):\n",
    "\n",
    "        # randomly choose noise\n",
    "        noise_num = torch.randint(low=0, high=len(\n",
    "            self.background_noises), size=(1,)).item()\n",
    "        noise = self.noises[noise_num]\n",
    "\n",
    "        noise_level = torch.Tensor([1])  # [0, 40]\n",
    "\n",
    "        noise_energy = torch.norm(noise)\n",
    "        audio_energy = torch.norm(audio)\n",
    "        alpha = (audio_energy / noise_energy) * \\\n",
    "            torch.pow(10, -noise_level / 20)\n",
    "\n",
    "        start = torch.randint(\n",
    "            low=0,\n",
    "            high=max(int(noise.size(0) - audio.size(0) - 1), 1),\n",
    "            size=(1,)\n",
    "        ).item()\n",
    "        noise_sample = noise[start: start + audio.size(0)]\n",
    "\n",
    "        audio_new = audio + alpha * noise_sample\n",
    "        audio_new.clamp_(-1, 1)\n",
    "        return audio_new\n",
    "\n",
    "    def __call__(self, wav):\n",
    "        aug_num = torch.randint(low=0, high=4, size=(1,)).item()   # choose 1 random aug from augs\n",
    "        augs = [\n",
    "            lambda x: x,\n",
    "            lambda x: (x + torch.distributions.Normal(0, 0.01).sample(x.size())).clamp_(-1, 1),\n",
    "            lambda x: torchaudio.transforms.Vol(.25)(x),\n",
    "            lambda x: self.add_rand_noise(x)\n",
    "        ]\n",
    "\n",
    "        return augs[aug_num](wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f45ccf7-aafa-46b0-9473-e27fbefae2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpeechCommandDataset(\n",
    "    path2dir='speech_commands', keywords=TaskConfig.keyword, part=\"train\", transform=AugsCreation()\n",
    ")\n",
    "val_dataset = SpeechCommandDataset(\n",
    "    path2dir='speech_commands', keywords=TaskConfig.keyword, part=\"val\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b336ca8-c552-43d6-8d76-e7398d921f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.index[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb86465-6cf1-40cb-9e45-102843000e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(data):\n",
    "    wavs = []\n",
    "    labels = []    \n",
    "\n",
    "    for el in data:\n",
    "        wavs.append(el['wav'])\n",
    "        labels.append(el['label'])\n",
    "\n",
    "    # torch.nn.utils.rnn.pad_sequence takes list(Tensors) and returns padded (with 0.0) Tensor\n",
    "    wavs = pad_sequence(wavs, batch_first=True)    \n",
    "    labels = torch.Tensor(labels).long()\n",
    "    return wavs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc73f86-847c-46cc-bb1c-df557e141748",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=TaskConfig.batch_size,\n",
    "                          shuffle=False, collate_fn=collate_fn,\n",
    "                          num_workers=2, pin_memory=True)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=TaskConfig.batch_size,\n",
    "                        shuffle=False, collate_fn=collate_fn,\n",
    "                        num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3797e444-a0ee-44a9-9d1a-d3c83e29f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130538c5-e66e-411a-9b7d-3ff978948365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogMelspec(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.melspec = torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=config.sample_rate,\n",
    "                n_fft=400,\n",
    "                win_length=400,\n",
    "                hop_length=160,\n",
    "                n_mels=config.n_mels\n",
    "        )\n",
    "\n",
    "        self.spec_augs = nn.Sequential(\n",
    "                torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "                torchaudio.transforms.TimeMasking(time_mask_param=35),\n",
    "        )\n",
    "\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        x = torch.log(self.melspec(batch).clamp_(min=1e-9, max=1e9))\n",
    "        if self.training:\n",
    "            x = self.spec_augs(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3474f4ea-910d-40b7-82ca-56650bff4ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommandDatasetV2(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform: Optional[Callable] = None,\n",
    "        path2dir: str = None,\n",
    "        keywords: Union[str, List[str]] = None,\n",
    "        csv: Optional[pd.DataFrame] = None,\n",
    "        part: \"str\" = \"train\",\n",
    "    ):        \n",
    "        self.transform = transform\n",
    "\n",
    "        self.path2dir = path2dir\n",
    "        self.keywords = keywords\n",
    "        self.index = self.create_or_load_index(part)\n",
    "\n",
    "    def create_or_load_index(self, part):\n",
    "        index_path = Path(f\"{part}_index.json\")\n",
    "        \n",
    "        if not index_path.exists():\n",
    "            self.create_index(part)\n",
    "            \n",
    "        return read_json(index_path)\n",
    "\n",
    "    def create_index(self, part):\n",
    "        path2dir = Path(self.path2dir)\n",
    "        keywords = self.keywords if isinstance(self.keywords, list) else [self.keywords]\n",
    "        \n",
    "        all_keywords = [\n",
    "            p.stem for p in path2dir.glob('*')\n",
    "            if p.is_dir() and not p.stem.startswith('_')\n",
    "        ]\n",
    "\n",
    "        index = []\n",
    "        for keyword in all_keywords:\n",
    "            paths = (path2dir / keyword).rglob('*.wav')\n",
    "            if keyword in keywords:\n",
    "                for path2wav in paths:\n",
    "                    index.append({\n",
    "                        \"path\": path2wav.as_posix(),\n",
    "                        \"keyword\": keyword,\n",
    "                        \"label\": 1\n",
    "                    })\n",
    "            else:\n",
    "                for path2wav in paths:\n",
    "                    index.append({\n",
    "                        \"path\": path2wav.as_posix(),\n",
    "                        \"keyword\": keyword,\n",
    "                        \"label\": 0\n",
    "                    })\n",
    "\n",
    "        torch.manual_seed(0)\n",
    "        indexes = torch.randperm(len(index))\n",
    "        train_indexes = indexes[:int(len(index) * 0.8)]\n",
    "        val_indexes = indexes[int(len(index) * 0.8):]\n",
    "\n",
    "        train_index = [index[i] for i in train_indexes]\n",
    "        val_index = [index[i] for i in val_indexes]\n",
    "\n",
    "        train_index_path = pathlib.Path(\"train_index.json\")\n",
    "        write_json(train_index, str(train_index_path))\n",
    "        \n",
    "        val_index_path = pathlib.Path(\"val_index.json\")\n",
    "        write_json(val_index, str(val_index_path))\n",
    "\n",
    "    def __getitem__(self, ind: int):\n",
    "        instance = self.index[ind]\n",
    "\n",
    "        path2wav = instance['path']\n",
    "        wav, sr = torchaudio.load(path2wav)\n",
    "        wav = wav.sum(dim=0)\n",
    "        \n",
    "        if self.transform:\n",
    "            wav = self.transform(wav)\n",
    "\n",
    "        return {\n",
    "            'wav': wav,\n",
    "            'keywors': instance['keyword'],\n",
    "            'label': instance['label']\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8be91-a439-42ad-bd77-77b5be595190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.energy = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        energy = self.energy(input)\n",
    "        alpha = torch.softmax(energy, dim=-2)\n",
    "        return (input * alpha).sum(dim=-2)\n",
    "\n",
    "class SpeechCommandDatasetV5(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform: Optional[Callable] = None,\n",
    "        path2dir: str = None,\n",
    "        keywords: Union[str, List[str]] = None,\n",
    "        csv: Optional[pd.DataFrame] = None,\n",
    "        part: \"str\" = \"train\",\n",
    "    ):        \n",
    "        self.transform = transform\n",
    "\n",
    "        self.path2dir = path2dir\n",
    "        self.keywords = keywords\n",
    "        self.index = self.create_or_load_index(part)\n",
    "\n",
    "    def create_or_load_index(self, part):\n",
    "        index_path = Path(f\"{part}_index.json\")\n",
    "        \n",
    "        if not index_path.exists():\n",
    "            self.create_index(part)\n",
    "            \n",
    "        return read_json(index_path)\n",
    "\n",
    "    def create_index(self, part):\n",
    "        path2dir = Path(self.path2dir)\n",
    "        keywords = self.keywords if isinstance(self.keywords, list) else [self.keywords]\n",
    "        \n",
    "        all_keywords = [\n",
    "            p.stem for p in path2dir.glob('*')\n",
    "            if p.is_dir() and not p.stem.startswith('_')\n",
    "        ]\n",
    "\n",
    "        index = []\n",
    "        for keyword in all_keywords:\n",
    "            paths = (path2dir / keyword).rglob('*.wav')\n",
    "            if keyword in keywords:\n",
    "                for path2wav in paths:\n",
    "                    index.append({\n",
    "                        \"path\": path2wav.as_posix(),\n",
    "                        \"keyword\": keyword,\n",
    "                        \"label\": 1\n",
    "                    })\n",
    "            else:\n",
    "                for path2wav in paths:\n",
    "                    index.append({\n",
    "                        \"path\": path2wav.as_posix(),\n",
    "                        \"keyword\": keyword,\n",
    "                        \"label\": 0\n",
    "                    })\n",
    "\n",
    "        torch.manual_seed(0)\n",
    "        indexes = torch.randperm(len(index))\n",
    "        train_indexes = indexes[:int(len(index) * 0.8)]\n",
    "        val_indexes = indexes[int(len(index) * 0.8):]\n",
    "\n",
    "        train_index = [index[i] for i in train_indexes]\n",
    "        val_index = [index[i] for i in val_indexes]\n",
    "\n",
    "        train_index_path = pathlib.Path(\"train_index.json\")\n",
    "        write_json(train_index, str(train_index_path))\n",
    "        \n",
    "        val_index_path = pathlib.Path(\"val_index.json\")\n",
    "        write_json(val_index, str(val_index_path))\n",
    "\n",
    "    def __getitem__(self, ind: int):\n",
    "        instance = self.index[ind]\n",
    "\n",
    "        path2wav = instance['path']\n",
    "        wav, sr = torchaudio.load(path2wav)\n",
    "        wav = wav.sum(dim=0)\n",
    "        \n",
    "        if self.transform:\n",
    "            wav = self.transform(wav)\n",
    "\n",
    "        return {\n",
    "            'wav': wav,\n",
    "            'keywors': instance['keyword'],\n",
    "            'label': instance['label']\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spec = LogMelspec(config)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.mel_spec(input)\n",
    "        \n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732e122-7988-4544-8815-c92de443a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNNV2(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.mel_spec = LogMelspec(config)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.mel_spec(input)\n",
    "        \n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab3ad0-c01c-4e2f-be1b-5a3c8c9ae227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10279b6-7547-47e1-a8ff-44d9a9feef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TaskConfig()\n",
    "model = CRNN(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275de3c2-9dfd-477b-98ba-e42e4ceef70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(train_dataset[0][\"wav\"].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e98516-e71b-4afe-9c2c-59ca6cbd7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e5fe8-f64e-43bd-a308-0c504cea4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59918ad1-3a74-4588-b26e-827d0e337fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRNNV2(...)\n",
    "\n",
    "model.to(config.device)\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS * len(train_dataloader), eta_min=1e-4)\n",
    "\n",
    "with wandb.init(\n",
    "                project=\"seminar_wandb_kws\", # project name\n",
    "                name=\"crnnv2\" # run name within the project\n",
    "            ) as run:\n",
    "    train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, config.device, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc333e1-65e5-4f2f-8b3b-825801d0c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FA - true: 0, model: 1\n",
    "# FR - true: 1, model: 0\n",
    "\n",
    "def count_fa(preds, labels):\n",
    "    preds = torch.argmax(preds, dim=-1)\n",
    "\n",
    "    FA = torch.sum(preds[labels == 0])\n",
    "    \n",
    "    # torch.numel - returns total number of elements in tensor\n",
    "    return FA.item() / torch.numel(preds)\n",
    "\n",
    "def count_fr(preds, labels):\n",
    "    preds = torch.argmax(preds, dim=-1)\n",
    "\n",
    "    FR = torch.sum(labels[preds == 0])\n",
    "    \n",
    "    # torch.numel - returns total number of elements in tensor\n",
    "    return FR.item() / torch.numel(preds)\n",
    "\n",
    "def count_acc(preds, labels):\n",
    "    preds = torch.argmax(preds, dim=-1)\n",
    "\n",
    "    acc = torch.sum(preds == labels)\n",
    "    \n",
    "    # torch.numel - returns total number of elements in tensor\n",
    "    return acc.item() / torch.numel(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d87ba59-936c-45b6-8468-2dd88c23a032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1fb606-b1f6-4f7e-b656-2523402950a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(config.device)\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS * len(train_dataloader), eta_min=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deffa1c5-bee8-441b-aee7-8d5eba4f9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(\n",
    "                project=\"seminar_wandb_kws\", # project name\n",
    "                name=\"crnn\" # run name within the project\n",
    "            ) as run:\n",
    "    train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, config.device, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9bdc8f-075f-4171-a3bf-3362159c8a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf78d5-80dc-417f-b444-5f30a031397f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31500c-c304-450d-8862-bde7a679f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRNNV3(...)\n",
    "\n",
    "model.to(config.device)\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS * len(train_dataloader), eta_min=1e-4)\n",
    "\n",
    "with wandb.init(\n",
    "                project=\"seminar_wandb_kws\", # project name\n",
    "                name=\"crnnv3\" # run name within the project\n",
    "            ) as run:\n",
    "    train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, config.device, NUM_EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
