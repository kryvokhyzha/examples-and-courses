{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631675f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from scipy import sparse\n",
    "import implicit\n",
    "from implicit.evaluation import precision_at_k, train_test_split\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f977e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir '/srv/data/vk'\n",
    "#!mkdir '/srv/data/vk/old'\n",
    "#!mkdir '/srv/data/vk/train'\n",
    "#!mkdir '/srv/data/vk/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10caf2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit.cpu.als import check_random_state, check_csr\n",
    "import time\n",
    "import logging\n",
    "log = logging.getLogger(\"implicit\")\n",
    "\n",
    "def fit(self, user_items, show_progress=True, callback=None):\n",
    "        \"\"\"Factorizes the user_items matrix.\n",
    "\n",
    "        After calling this method, the members 'user_factors' and 'item_factors' will be\n",
    "        initialized with a latent factor model of the input data.\n",
    "\n",
    "        The user_items matrix does double duty here. It defines which items are liked by which\n",
    "        users (P_ui in the original paper), as well as how much confidence we have that the user\n",
    "        liked the item (C_ui).\n",
    "\n",
    "        The negative items are implicitly defined: This code assumes that positive items in the\n",
    "        user_items matrix means that the user liked the item. The negatives are left unset in this\n",
    "        sparse matrix: the library will assume that means Piu = 0 and Ciu = 1 for all these items.\n",
    "        Negative items can also be passed with a higher confidence value by passing a negative\n",
    "        value, indicating that the user disliked the item.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        user_items: csr_matrix\n",
    "            Matrix of confidences for the liked items. This matrix should be a csr_matrix where\n",
    "            the rows of the matrix are the users, the columns are the items liked that user,\n",
    "            and the value is the confidence that the user liked the item.\n",
    "        show_progress : bool, optional\n",
    "            Whether to show a progress bar during fitting\n",
    "        callback: Callable, optional\n",
    "            Callable function on each epoch with such arguments as epoch, elapsed time and progress\n",
    "        \"\"\"\n",
    "        # initialize the random state\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        Cui = check_csr(user_items)\n",
    "        if Cui.dtype != np.float32:\n",
    "            Cui = Cui.astype(np.float32)\n",
    "\n",
    "        # Give the positive examples more weight if asked for\n",
    "        if self.alpha != 1.0:\n",
    "            Cui = self.alpha * Cui\n",
    "\n",
    "        s = time.time()\n",
    "        Ciu = Cui.T.tocsr()\n",
    "        log.debug(\"Calculated transpose in %.3fs\", time.time() - s)\n",
    "\n",
    "        items, users = Ciu.shape\n",
    "\n",
    "        s = time.time()\n",
    "        # Initialize the variables randomly if they haven't already been set\n",
    "        if self.user_factors is None:\n",
    "            self.user_factors = random_state.rand(users, self.factors).astype(self.dtype) * 0.01\n",
    "        if self.item_factors is None:\n",
    "            self.item_factors = random_state.rand(items, self.factors).astype(self.dtype) * 0.01\n",
    "\n",
    "        log.debug(\"Initialized factors in %s\", time.time() - s)\n",
    "\n",
    "        # invalidate cached norms and squared factors\n",
    "        self._item_norms = self._user_norms = None\n",
    "        self._YtY = None\n",
    "        self._XtX = None\n",
    "        loss = None\n",
    "\n",
    "        solver = self.solver\n",
    "\n",
    "        log.debug(\"Running %i ALS iterations\", self.iterations)\n",
    "        with tqdm(total=self.iterations, disable=not show_progress) as progress:\n",
    "            # alternate between learning the user_factors from the item_factors and vice-versa\n",
    "            for iteration in range(self.iterations):\n",
    "                s = time.time()\n",
    "                solver(\n",
    "                    Cui,\n",
    "                    self.user_factors,\n",
    "                    self.item_factors,\n",
    "                    self.regularization,\n",
    "                    num_threads=self.num_threads,\n",
    "                )\n",
    "                if False:\n",
    "                    solver(\n",
    "                        Ciu,\n",
    "                        self.item_factors,\n",
    "                        self.user_factors,\n",
    "                        self.regularization,\n",
    "                        num_threads=self.num_threads,\n",
    "                    )\n",
    "                progress.update(1)\n",
    "\n",
    "                if self.calculate_training_loss:\n",
    "                    loss = _als.calculate_loss(\n",
    "                        Cui,\n",
    "                        self.user_factors,\n",
    "                        self.item_factors,\n",
    "                        self.regularization,\n",
    "                        num_threads=self.num_threads,\n",
    "                    )\n",
    "                    progress.set_postfix({\"loss\": loss})\n",
    "\n",
    "                    if not show_progress:\n",
    "                        log.info(\"loss %.4f\", loss)\n",
    "\n",
    "                # Backward compatibility\n",
    "                if not callback:\n",
    "                    callback = self.fit_callback\n",
    "                if callback:\n",
    "                    callback(iteration, time.time() - s, loss)\n",
    "\n",
    "        if self.calculate_training_loss:\n",
    "            log.info(\"Final training loss %.4f\", loss)\n",
    "\n",
    "        self._check_fit_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c00c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureMaking:\n",
    "    \n",
    "    def __init__(self, path_to_save, iloc_start, iloc_end=None, production_flg = False):\n",
    "        self.path_to_save = path_to_save\n",
    "        self.iloc_start = iloc_start\n",
    "        self.iloc_end = iloc_end\n",
    "        self.production_flg = production_flg \n",
    "        \n",
    "        \n",
    "    def load_data(self, load_path = './'):\n",
    "        full_df = pd.read_parquet(os.path.join(load_path,'train.parquet.gzip'))\n",
    "        self.item_df = pd.read_parquet(os.path.join(load_path,'items_meta.parquet.gzip'))\n",
    "        self.item_source_dct = self.item_df.set_index('item_id')['source_id'].to_dict()\n",
    "        full_df['source_id'] = full_df['item_id'].map(self.item_source_dct)\n",
    "\n",
    "        if self.production_flg:\n",
    "            self.train_df = full_df.reset_index(drop=True)\n",
    "            self.target_df = None\n",
    "            self.candidates_df = pd.read_parquet(os.path.join(load_path,'fresh_candidates.parquet.gzip'))\n",
    "            self.test_user_df = pd.read_parquet(os.path.join(load_path,'test.parquet.gzip'))\n",
    "            \n",
    "        else:\n",
    "            self.train_df = full_df.iloc[:self.iloc_start].reset_index(drop=True)\n",
    "            \n",
    "            if self.iloc_end is None:\n",
    "                self.target_df = full_df.iloc[self.iloc_start:].reset_index(drop=True)\n",
    "            else:\n",
    "                self.target_df = full_df.iloc[self.iloc_start:self.iloc_end].reset_index(drop=True)\n",
    "            self.candidates_df = self.target_df[['item_id']].drop_duplicates().reset_index(drop=True)\n",
    "            self.test_user_df = self.target_df[['user_id']].drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.train_df['reaction_abs'] = np.abs(self.train_df['reaction']).astype('float32')\n",
    "        self.train_df['good'] = (self.train_df['timespent']>0).astype('int')\n",
    "        self.train_df = self.train_df.reset_index()\n",
    "        self.train_df['good'] = (self.train_df['timespent']>0).astype('int')\n",
    "        \n",
    "    def get_feature_source_df(self, n_last_row = 1000000):\n",
    "        filepath = os.path.join(self.path_to_save,'feature_source_df.parquet.gzip')\n",
    "        try:\n",
    "            feature_source_df = pd.read_parquet(filepath)\n",
    "        except FileNotFoundError:\n",
    "            feature_source_df = self.train_df.iloc[-n_last_row:].groupby('source_id').agg({'good':('mean','sum')})\n",
    "            feature_source_df.columns = ['source_good_mean', 'source_good_sum']\n",
    "            feature_source_df.reset_index().to_parquet(filepath, compression='gzip')\n",
    "        return feature_source_df\n",
    "    \n",
    "    def get_feature_source_user_df(self):\n",
    "        filepath = os.path.join(self.path_to_save,'feature_source_user_df.parquet.gzip')\n",
    "        try:\n",
    "            feature_source_user_df = pd.read_parquet(filepath)\n",
    "        except FileNotFoundError:\n",
    "            feature_source_user_df = self.train_df[train_df['user_id'].isin(self.test_user_df.user_id)].groupby(['user_id','source_id']).agg({\n",
    "                    'item_id':'nunique',\n",
    "                    'timespent':'sum',\n",
    "                    'good':('mean','sum'),\n",
    "                    'reaction':'mean',\n",
    "                    'reaction_abs':('mean','sum')})\n",
    "            feature_source_user_df.columns = ['cnt_items',\n",
    "                                          'time_sum',\n",
    "                                          'good_mean',\n",
    "                                          'good_sum',\n",
    "                                          'reaction_mean',\n",
    "                                          'reaction_abs_mean',\n",
    "                                          'reaction_abs_sum']\n",
    "    \n",
    "            feature_source_user_df.reset_index().to_parquet(filepath, compression='gzip')\n",
    "\n",
    "        return feature_source_user_df\n",
    "    \n",
    "    def get_feature_item_df(self):\n",
    "        filepath = os.path.join(self.path_to_save,'feature_item_df.parquet.gzip')\n",
    "        try:\n",
    "            feature_item_df = pd.read_parquet(filepath)\n",
    "        except FileNotFoundError:\n",
    "            feature_item_df = train_df.groupby('item_id').agg({'user_id':'nunique',\n",
    "                                             'timespent':'mean',\n",
    "                                             'good':'mean',\n",
    "                                             'reaction_abs':'mean'}).reset_index().rename(\n",
    "                columns = {'user_id':'cnt_users_by_item',\n",
    "                           'timespent':'mean_time_by_item',\n",
    "                           'good':'mean_good_by_item',\n",
    "                           'reaction_abs':'mean_abs_react_by_item'})\n",
    "            feature_item_df['mean_abs_react_by_item'] = feature_item_df['mean_abs_react_by_item'].astype('float32')\n",
    "            feature_item_df.to_parquet(filepath, compression='gzip')\n",
    "    \n",
    "\n",
    "\n",
    "        feature_item_df['pretarget_time_sum_5m'] = feature_item_df['item_id'].map(\n",
    "            self.train_df.iloc[-5000000:].groupby('item_id')['timespent'].sum().to_dict()).fillna(0)\n",
    "\n",
    "        feature_item_df['pretarget_time_sum_1m'] = feature_item_df['item_id'].map(\n",
    "            self.train_df.iloc[-1000000:].groupby('item_id')['timespent'].sum().to_dict()).fillna(0)\n",
    "\n",
    "        feature_item_df['pretarget_good_sum_5m'] = feature_item_df['item_id'].map(\n",
    "            self.train_df.iloc[-5000000:].groupby('item_id')['good'].sum().to_dict()).fillna(0)\n",
    "\n",
    "        feature_item_df['pretarget_good_sum_1m'] = feature_item_df['item_id'].map(\n",
    "            self.train_df.iloc[-1000000:].groupby('item_id')['good'].sum().to_dict()).fillna(0)\n",
    "\n",
    "\n",
    "        feature_item_df['pretarget_prc'] = feature_item_df['pretarget_time_sum_1m']/(feature_item_df[\n",
    "            'pretarget_time_sum_5m']+0.1)\n",
    "\n",
    "        feature_item_df['source_id'] = feature_item_df['item_id'].map(self.item_source_dct)\n",
    "        \n",
    "        return feature_item_df\n",
    "    \n",
    "    def get_feature_item_emb_cosine_df(self, top_feature_item_df):\n",
    "        filepath = os.path.join(self.path_to_save,'feature_item_emb_cosine_df.parquet.gzip')\n",
    "        try:\n",
    "            feature_item_emb_cosine_df = pd.read_parquet(filepath)\n",
    "        except FileNotFoundError:\n",
    "            embedding_dct = item_df.set_index('item_id')['embeddings'].apply(np.array).to_dict()\n",
    "            users_by_item_dct = train_df[train_df['timespent']>0].groupby('item_id')['user_id'].apply(set).to_dict()\n",
    "            item_arr = train_df[train_df['timespent']>0]['item_id'].unique()\n",
    "\n",
    "            embed_matrix_lst = []\n",
    "            for item_id in item_arr:\n",
    "                embed_matrix_lst.append(embedding_dct[item_id])\n",
    "            embed_matrix = np.array(embed_matrix_lst)\n",
    "\n",
    "            recommend_item_df_lst = []\n",
    "\n",
    "            for item_id_arr in tqdm(np.array_split(top_feature_item_df.item_id.values,50)):\n",
    "                matrix_one = np.array([embedding_dct[item_id] for item_id in item_id_arr])\n",
    "                one_cosine = cosine_similarity(matrix_one,embed_matrix)\n",
    "\n",
    "                for i, item_id in enumerate(item_id_arr):\n",
    "                    tmp_df = pd.DataFrame({'item_recommend_id':item_id,\n",
    "                              'item_id': item_arr, \n",
    "                              'cosine':one_cosine[i]}).sort_values(\n",
    "                    'cosine', ascending = False)\n",
    "                    recommend_item_df_lst.append(tmp_df[tmp_df['cosine']>0.9])\n",
    "\n",
    "            recom_df = pd.concat(recommend_item_df_lst).reset_index(drop=True)\n",
    "            recom_df = recom_df[recom_df['item_recommend_id']!=recom_df['item_id']].reset_index(drop=True)\n",
    "\n",
    "            full_recom_lst = []\n",
    "            for item_id_arr in tqdm(np.array_split(top_feature_item_df.item_id.values,100)):\n",
    "                tmp_df = recom_df[recom_df['item_recommend_id'].isin(item_id_arr)].merge(\n",
    "                    train_df[train_df['timespent']>0][['item_id','user_id']]).sort_values(\n",
    "                        'cosine', ascending = False).groupby(['item_recommend_id','user_id']).head(1)[\n",
    "                        ['item_recommend_id','user_id','cosine']].rename(columns = {'item_recommend_id':'item_id'})\n",
    "                full_recom_lst.append(tmp_df)\n",
    "\n",
    "            full_recom_df = pd.concat(full_recom_lst).sort_values(\n",
    "                    'cosine', ascending = False).groupby(['item_id','user_id']).head(1).reset_index(drop=True)\n",
    "\n",
    "            full_recom_df[['item_id','user_id','cosine']].to_parquet(filepath, compression='gzip')\n",
    "            feature_item_emb_cosine_df = full_recom_df[['item_id','user_id','cosine']]\n",
    "\n",
    "        return feature_item_emb_cosine_df\n",
    "        \n",
    "    def get_best_items(self, feature_item_df):\n",
    "        filepath = os.path.join(self.path_to_save,'best_150_df.parquet.gzip')\n",
    "        try:\n",
    "            best_150_df = pd.read_parquet(filepath)\n",
    "        except FileNotFoundError:\n",
    "        \n",
    "            best_item_df = feature_item_df[feature_item_df['item_id'].isin(self.candidates_df.item_id)].sort_values(\n",
    "                'pretarget_time_sum_5m', ascending = False).reset_index(drop=True).head(150)\n",
    "            best_item_df['flg'] = 1\n",
    "            target_user_df = self.test_user_df[['user_id']].drop_duplicates().reset_index(drop=True)\n",
    "            target_user_df['flg'] = 1\n",
    "            best_150_df = best_item_df.merge(target_user_df)[['user_id','item_id']]\n",
    "            best_150_df.to_parquet(filepath, compression='gzip')\n",
    "\n",
    "        return best_150_df\n",
    "    \n",
    "    \n",
    "    def get_feature_als_df(self):\n",
    "        filepath = os.path.join(self.path_to_save,'feature_als_512_15_df.parquet.gzip')\n",
    "        try:\n",
    "            feature_als_df = pd.read_parquet(filepath)\n",
    "        except FileNotFoundError:\n",
    "\n",
    "            users = train_df['user_id'].unique()\n",
    "            items = train_df['item_id'].unique()\n",
    "            shape = (len(users), len(items))\n",
    "\n",
    "            # Create indices for users and movies\n",
    "            user_cat = CategoricalDtype(categories=sorted(users), ordered=True)\n",
    "            items_cat = CategoricalDtype(categories=sorted(items), ordered=True)\n",
    "            user_index = train_df['user_id'].astype(user_cat).cat.codes\n",
    "            item_index = train_df['item_id'].astype(items_cat).cat.codes\n",
    "            user_item_rating_csr = sparse.coo_matrix((self.train_df[\"timespent\"]+1, \n",
    "                                                      (user_index, item_index)), shape=shape).tocsr()\n",
    "            from_user_id_to_index_dct = {}\n",
    "            for i, k in enumerate(user_cat.categories):\n",
    "                from_user_id_to_index_dct[k] = i\n",
    "\n",
    "            from_index_to_user_id_dct = {}\n",
    "            for i, k in enumerate(user_cat.categories):\n",
    "                from_index_to_user_id_dct[i] = k\n",
    "\n",
    "            from_index_to_item_id_dct = {}\n",
    "            for i, k in enumerate(items_cat.categories):\n",
    "                from_index_to_item_id_dct[i] = k\n",
    "\n",
    "\n",
    "            #items_to_predict = target_df[target_df['item_id'].isin(items)].item_id.unique()\n",
    "\n",
    "            items_to_predict = target_df[target_df['item_id'].isin(items)].item_id.unique()\n",
    "            items_to_predict_ind = pd.Series(items_to_predict).astype(items_cat).cat.codes.values\n",
    "\n",
    "            from_index_predict_to_item_id_dct = {}\n",
    "            for i in range(len(items_to_predict)):\n",
    "                from_index_predict_to_item_id_dct[i] = items_to_predict[i]\n",
    "\n",
    "            als_model = AlternatingLeastSquares(factors=512,\n",
    "                                                use_gpu=False,\n",
    "                                                regularization=0.1,\n",
    "                                                alpha=1,\n",
    "                                                iterations=15)\n",
    "            als_model.fit(user_item_rating_csr)\n",
    "\n",
    "            sample_users = self.test_user_df[self.test_user_df['user_id'].isin(users)].user_id.unique()\n",
    "            user_ind = [from_user_id_to_index_dct[user_id] for user_id in sample_users]\n",
    "            recom_result = als_model.recommend(\n",
    "                        user_ind,\n",
    "                        user_item_rating_csr[user_ind],\n",
    "                        N=100,\n",
    "                        recalculate_user = False,\n",
    "                        filter_already_liked_items=True,\n",
    "                        items = items_to_predict_ind\n",
    "                        )\n",
    "\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            als_df_lst = []\n",
    "            for i, user_id in tqdm(enumerate(sample_users)):\n",
    "                tmp_df = pd.DataFrame({'user_id':user_id,\n",
    "                                       'item_id':[from_index_to_item_id_dct[a] for a in recom_result[0][i]],\n",
    "                                       'als_score':recom_result[1][i]})\n",
    "                als_df_lst.append(tmp_df)\n",
    "\n",
    "            feature_als_df = pd.concat(als_df_lst).reset_index(drop=True)\n",
    "            feature_als_df.to_parquet(filepath, compression='gzip')\n",
    "        \n",
    "        return feature_als_df\n",
    "    \n",
    "    def get_feature_emb_als_tune_df(self):\n",
    "        filepath = os.path.join(self.path_to_save,'feature_emb_als_df_3_3.parquet.gzip')\n",
    "        try:\n",
    "            feature_emb_als_df = pd.read_parquet(filepath)\n",
    "        except FileNotFoundError:\n",
    "        \n",
    "            embedding_dct = item_df.set_index('item_id')['embeddings'].to_dict()\n",
    "            embed_matrix_lst = []\n",
    "            for i in range(len(items_cat.categories)):\n",
    "                embed_matrix_lst.append(np.array(embedding_dct[from_index_to_item_id_dct[i]]))\n",
    "\n",
    "            embed_matrix = np.array(embed_matrix_lst)\n",
    "\n",
    "            als_embedding_model = implicit.als.AlternatingLeastSquares(factors=312,\n",
    "                                        use_gpu=False,\n",
    "                                        regularization=0.1,\n",
    "                                        alpha=1.0,\n",
    "                                        iterations=3)\n",
    "\n",
    "            als_embedding_model.item_factors = embed_matrix.copy()\n",
    "\n",
    "            fit(als_embedding_model, user_item_rating_csr)\n",
    "            als_embedding_model.fit(user_item_rating_csr)\n",
    "\n",
    "            sample_users = target_df[target_df['user_id'].isin(users)].user_id.unique()\n",
    "            user_ind = [from_user_id_to_index_dct[user_id] for user_id in sample_users]\n",
    "            als_emb_recom_result = als_embedding_model.recommend(\n",
    "                        user_ind,\n",
    "                        user_item_rating_csr[user_ind],\n",
    "                        N=100,\n",
    "                        recalculate_user = False,\n",
    "                        filter_already_liked_items=True,\n",
    "                        items = items_to_predict_ind\n",
    "                        )\n",
    "\n",
    "            emb_als_df_lst = []\n",
    "            for i, user_id in tqdm(enumerate(sample_users)):\n",
    "                tmp_df = pd.DataFrame({'user_id':user_id,\n",
    "                                       'item_id':[from_index_to_item_id_dct[a] for a in als_emb_recom_result[0][i]],\n",
    "                                       'emb_als_score':als_emb_recom_result[1][i]})\n",
    "                emb_als_df_lst.append(tmp_df)\n",
    "\n",
    "            feature_emb_als_df = pd.concat(emb_als_df_lst).reset_index(drop=True)\n",
    "            feature_emb_als_df.to_parquet(filepath, compression='gzip')\n",
    "            \n",
    "        return feature_emb_als_df.rename(columns = {'emb_als_score':'emb_als_score_tune'})\n",
    "    \n",
    "    \n",
    "    def get_feature_emb_als_df(self):\n",
    "        filepath = os.path.join(self.path_to_save,'feature_emb_als_df.parquet.gzip')\n",
    "        try:\n",
    "            feature_emb_als_df = pd.read_parquet(filepath)\n",
    "        except FileNotFoundError:\n",
    "        \n",
    "            embedding_dct = item_df.set_index('item_id')['embeddings'].to_dict()\n",
    "            embed_matrix_lst = []\n",
    "            for i in range(len(items_cat.categories)):\n",
    "                embed_matrix_lst.append(np.array(embedding_dct[from_index_to_item_id_dct[i]]))\n",
    "\n",
    "            embed_matrix = np.array(embed_matrix_lst)\n",
    "\n",
    "            als_embedding_model = AlternatingLeastSquares(factors=312,\n",
    "                                        use_gpu=False,\n",
    "                                        regularization=0.1,\n",
    "                                        alpha=1.0,\n",
    "                                        iterations=15)\n",
    "\n",
    "            als_embedding_model.item_factors = embed_matrix.copy()\n",
    "\n",
    "            fit(als_embedding_model, user_item_rating_csr)\n",
    "\n",
    "            sample_users = target_df[target_df['user_id'].isin(users)].user_id.unique()\n",
    "            user_ind = [from_user_id_to_index_dct[user_id] for user_id in sample_users]\n",
    "            als_emb_recom_result = als_embedding_model.recommend(\n",
    "                        user_ind,\n",
    "                        user_item_rating_csr[user_ind],\n",
    "                        N=100,\n",
    "                        recalculate_user = False,\n",
    "                        filter_already_liked_items=True,\n",
    "                        items = items_to_predict_ind\n",
    "                        )\n",
    "\n",
    "            emb_als_df_lst = []\n",
    "            for i, user_id in tqdm(enumerate(sample_users)):\n",
    "                tmp_df = pd.DataFrame({'user_id':user_id,\n",
    "                                       'item_id':[from_index_to_item_id_dct[a] for a in als_emb_recom_result[0][i]],\n",
    "                                       'emb_als_score':als_emb_recom_result[1][i]})\n",
    "                emb_als_df_lst.append(tmp_df)\n",
    "\n",
    "            feature_emb_als_df = pd.concat(emb_als_df_lst).reset_index(drop=True)\n",
    "            feature_emb_als_df.to_parquet(filepath, compression='gzip')\n",
    "            \n",
    "        return feature_emb_als_df\n",
    "    \n",
    "    def get_top_feature_item_df(self,feature_item_df):\n",
    "        tmp_df = self.train_df.iloc[-500000:].copy().reset_index(drop=True)\n",
    "        top_feature_item_df = feature_item_df[feature_item_df['item_id'].isin(\n",
    "            tmp_df[tmp_df['timespent']>0]['item_id'].unique())]\n",
    "        return top_feature_item_df\n",
    "    \n",
    "    def get_test_users(self):\n",
    "        return self.test_user_df.user_id.unique()\n",
    "    \n",
    "    def get_train_df(self):\n",
    "        return self.train_df\n",
    "    \n",
    "    def get_production_flg(self):\n",
    "        return self.production_flg\n",
    "    \n",
    "    def get_target_df(self):\n",
    "        return self.target_df\n",
    "    \n",
    "    def get_path_to_save(self):\n",
    "        return self.path_to_save\n",
    "    \n",
    "    def make_result_df(self, input_df, feature_item_df, feature_als_df, feature_source_df,\n",
    "                       feature_emb_als_df, feature_emb_als_tune_df, feature_item_emb_cosine_df, \n",
    "                       feature_source_user_df):\n",
    "        result_df = input_df[['user_id','item_id']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        train_df = self.train_df\n",
    "\n",
    "        train_df['already'] = 1\n",
    "        result_df = result_df.merge(\n",
    "                train_df[train_df['user_id'].isin(result_df['user_id'].unique()) &\n",
    "                         train_df['item_id'].isin(result_df['item_id'])][\n",
    "                    ['user_id','item_id','already']], how = 'left').fillna(0).reset_index(drop=True)\n",
    "\n",
    "        result_df = result_df[result_df['already']==0].drop('already', axis = 1).reset_index(drop=True)\n",
    "        result_df['source_id'] = result_df['item_id'].map(self.item_source_dct)\n",
    "\n",
    "\n",
    "        result_df = result_df.merge(feature_item_df[feature_item_df['item_id'].isin(result_df['item_id'])], how = 'left') \\\n",
    "                 .merge(feature_source_user_df[\n",
    "                         feature_source_user_df['user_id'].isin(result_df['user_id'].unique()) &\n",
    "                         feature_source_user_df['source_id'].isin(result_df['source_id'])], \n",
    "                        how = 'left').reset_index(drop=True).fillna(0)\n",
    "\n",
    "        result_df = result_df.merge(feature_als_df[feature_als_df['user_id'].isin(result_df['user_id'])], how = 'left') \\\n",
    "                             .merge(feature_emb_als_df[feature_emb_als_df['user_id'].isin(result_df['user_id'])], \n",
    "                                    how = 'left').fillna(0).reset_index(drop=True)\n",
    "        result_df = result_df.merge(feature_emb_als_tune_df[feature_emb_als_tune_df['user_id'].isin(\n",
    "            result_df['user_id'])], how = 'left').fillna(0).reset_index(drop=True)\n",
    "        result_df = result_df.merge(feature_item_emb_cosine_df[feature_item_emb_cosine_df['user_id'].isin(result_df['user_id'])], \n",
    "                                    how = 'left').fillna(0).reset_index(drop=True)\n",
    "\n",
    "        result_df['source_good_mean'] = result_df['source_id'].map(\n",
    "            feature_source_df.set_index('source_id')['source_good_mean'].to_dict()).fillna(0)\n",
    "        result_df['source_good_sum'] = result_df['source_id'].map(\n",
    "            feature_source_df.set_index('source_id')['source_good_sum'].to_dict()).fillna(0)\n",
    "\n",
    "        if not self.production_flg:\n",
    "            result_df = result_df.merge(self.target_df[self.target_df['timespent']>0][\n",
    "                ['user_id','item_id','timespent']], how = 'left').fillna(0)\n",
    "\n",
    "        return result_df\n",
    "    \n",
    "    \n",
    "    def main(self, data_path = './'):\n",
    "        self.load_data(data_path)\n",
    "        \n",
    "        # features of source_id\n",
    "        feature_source_df = self.get_feature_source_df()\n",
    "        # features of pairs user_id - source_id\n",
    "        feature_source_user_df = self.get_feature_source_user_df()\n",
    "        # features of items\n",
    "        feature_item_df = self.get_feature_item_df()\n",
    "        # als pairs user_id, item_id, score\n",
    "        feature_als_df = self.get_feature_als_df()\n",
    "        # als pairs user_id, item_id, score fit from embedding vectors\n",
    "        feature_emb_als_df = self.get_feature_emb_als_df()\n",
    "\n",
    "        # other als pairs user_id, item_id, score fit from embedding vectors\n",
    "        feature_emb_als_tune_df = self.get_feature_emb_als_tune_df()\n",
    "\n",
    "        # pairs user_id - item_id for each best items\n",
    "        best_150_df = self.get_best_items(feature_item_df)\n",
    "\n",
    "\n",
    "        top_feature_item_df = self.get_top_feature_item_df(feature_item_df)\n",
    "\n",
    "        # all pairs user_id - item_id for sources with positive timespents\n",
    "        full_train_item_df = feature_source_user_df[feature_source_user_df['good_sum']>0].merge(\n",
    "            top_feature_item_df)\n",
    "\n",
    "        # searching best candidates by cosine similarity of embedding vectors\n",
    "        feature_item_emb_cosine_df = self.get_feature_item_emb_cosine_df(top_feature_item_df)\n",
    "        \n",
    "        \n",
    "        input_df = pd.concat([best_150_df[['user_id','item_id']], \n",
    "                      full_train_item_df[['user_id','item_id']],\n",
    "                      feature_als_df[['user_id','item_id']],\n",
    "                      feature_emb_als_tune_df[['user_id','item_id']],\n",
    "                      feature_emb_als_df[['user_id','item_id']],\n",
    "                      feature_item_emb_cosine_df[['user_id','item_id']]]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        input_df = input_df[input_df['user_id'].isin(self.get_test_users())].reset_index(drop=True)\n",
    "        print(input_df.shape)\n",
    "        \n",
    "        user_array = self.get_test_users()\n",
    "        np.random.seed(33)\n",
    "        np.random.shuffle(user_array)\n",
    "\n",
    "        user_lst = np.array_split(user_array,10)\n",
    "\n",
    "        pretrain_users = list(user_lst[0])+list(user_lst[1])\n",
    "        train_users = list(user_lst[2])+list(user_lst[3])+list(user_lst[4])+list(user_lst[5])+list(user_lst[6])+list(user_lst[7])+list(user_lst[8])\n",
    "        valid_users = list(user_lst[9])\n",
    "\n",
    "\n",
    "        i = 0\n",
    "        for tmp_user_arr in tqdm(user_lst):\n",
    "            save_filepath = os.path.join(self.path_to_save, f'result_df_{i}.parquet.gzip')\n",
    "            tmp_result_df = self.make_result_df(input_df[input_df['user_id'].isin(tmp_user_arr)], \n",
    "                                               feature_item_df, feature_als_df, feature_source_df,\n",
    "                                           feature_emb_als_df, feature_emb_als_tune_df, feature_item_emb_cosine_df,\n",
    "                                                feature_source_user_df)\n",
    "            tmp_result_df = tmp_result_df.sort_values('user_id').reset_index(drop=True)\n",
    "            tmp_result_df.to_parquet(save_filepath, compression='gzip')\n",
    "            i+=1\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8789a3a",
   "metadata": {},
   "source": [
    "# SAVE STUDY DATASET ON TRAIN DF ENDING on -10m rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845fa04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330649346, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [25:51<00:00, 155.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25min 42s, sys: 3min 26s, total: 29min 9s\n",
      "Wall time: 28min 36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fm_old = FeatureMaking(path_to_save = '/srv/data/vk/old', \n",
    "                   iloc_start = -10000000, \n",
    "                   iloc_end = -5000000, \n",
    "                   production_flg = False)\n",
    "fm_old.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1115a0",
   "metadata": {},
   "source": [
    "# SAVE STUDY DATASET ON TRAIN DF ENDING on -5m rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8086bb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296480432, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [22:34<00:00, 135.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 11s, sys: 3min 8s, total: 25min 20s\n",
      "Wall time: 24min 38s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fm = FeatureMaking(path_to_save = '/srv/data/vk/train', \n",
    "                   iloc_start = -5000000, \n",
    "                   iloc_end = None, \n",
    "                   production_flg = False)\n",
    "fm.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a8c46c",
   "metadata": {},
   "source": [
    "# SAVE STUDY DATASET ON TRAIN DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ccbde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132615483, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [09:12<00:00, 55.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 3s, sys: 1min 18s, total: 10min 21s\n",
      "Wall time: 10min 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_fm = FeatureMaking(path_to_save = '/srv/data/vk/test', \n",
    "                   iloc_start = None, \n",
    "                   iloc_end = None, \n",
    "                   production_flg = True)\n",
    "test_fm.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599f577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
