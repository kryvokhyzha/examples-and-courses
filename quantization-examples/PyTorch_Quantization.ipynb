{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch Quantization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okhqb73zWWev"
      },
      "source": [
        "# PyTorch Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MuBbPc1WgDe"
      },
      "source": [
        "Quantization - is a process of mapping values from bigget set to the smaller one. \n",
        "In ML this process is done for 2 reason:\n",
        "1. Lower model size. In most cases size of the model decrese in 2-6 times.\n",
        "2. Make model to inference faster on CPU, because of limmiting amount of data, and more simpler and faster operations on int8.\n",
        "\n",
        "In our case we will be mapping values from float32(-3.4E+38 to +3.4E+38) to the int8(-128 to +128). As you can see difference is quite big and that's means that we will get accuracy drops(but not always, sometimes small models can increase accuracy, some sort of regularization)\n",
        "\n",
        "There are two ways of model quantization:\n",
        "1. Post-training quantization\n",
        "2. Quantization-aware training\n",
        "\n",
        "Today we will cover both of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgX05CneYie_"
      },
      "source": [
        "## Post-training quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj8TzBfI3TXP"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jotONn6w3qvQ"
      },
      "source": [
        "cd apex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqvU38ZA3rpj"
      },
      "source": [
        "# for real use please use installation with CUDA and CPP\n",
        "!pip install -v --no-cache-dir ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qobVLKkq5PMP"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrVLt9sKHFMs"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "from torchvision.models.resnet import Bottleneck, BasicBlock, ResNet, model_urls\n",
        "import torch.nn as nn\n",
        "from torchvision.models.utils import load_state_dict_from_url\n",
        "from torch.quantization import QuantStub, DeQuantStub, fuse_modules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKMoZ5B3WPW1"
      },
      "source": [
        "def make_divisible(v, divisor, min_value=None):\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "class QuantizableBasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    __constants__ = ['downsample']\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, groups=None, base_width=None, previous_dilation=None, norm_layer=None, dilation=None):\n",
        "        super(QuantizableBasicBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels, momentum=0.1)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels, momentum=0.1)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        # used to wrap some simple float operations like add, mul, relu, etc.\n",
        "        self.skip_add_relu = torch.nn.quantized.FloatFunctional()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out = self.skip_add_relu.add_relu(out, identity)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def fuse_model(self):\n",
        "        torch.quantization.fuse_modules(self, [['conv1', 'bn1', 'relu'],\n",
        "                                               ['conv2', 'bn2']], inplace=True)\n",
        "        if self.downsample:\n",
        "            torch.quantization.fuse_modules(self.downsample, ['0', '1'], inplace=True)\n",
        "\n",
        "class QuantizableBottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    __constants__ = ['downsample']\n",
        "\n",
        "    def __init__(self, in_channels, channels, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None, **kwargs):\n",
        "        super(QuantizableBottleneck, self).__init__()\n",
        "\n",
        "        width = make_divisible(int(channels * (base_width / 64.)) * groups, 8)\n",
        "        self.conv1 = nn.Conv2d(in_channels, width, 1, stride=stride, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(width, momentum=0.1)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(width, width, 3, stride=stride, padding=1, groups=groups, dilation=dilation, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(width, momentum=0.1)\n",
        "\n",
        "        out_channels = make_divisible(channels * self.expansion, 8)\n",
        "        self.conv3 = nn.Conv2d(width, out_channels, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels, momentum=0.1)\n",
        "\n",
        "        self.relu1 = nn.ReLU(inplace=False)\n",
        "        self.relu2 = nn.ReLU(inplace=False)\n",
        "\n",
        "        self.skip_add_relu = nn.quantized.FloatFunctional()\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out = self.skip_add_relu.add_relu(out, identity)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def fuse_model(self):\n",
        "        torch.quantization.fuse_modules(self, [['conv1', 'bn1', 'relu1'],\n",
        "                                               ['conv2', 'bn2', 'relu2'],\n",
        "                                               ['conv3', 'bn3']], inplace=True)\n",
        "        if self.downsample:\n",
        "            torch.quantization.fuse_modules(self.downsample, ['0', '1'], inplace=True)\n",
        "\n",
        "class QuantizableResNet(ResNet):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(QuantizableResNet, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        x = self._forward_impl(x)\n",
        "        x = self.dequant(x)\n",
        "        return x\n",
        "\n",
        "    def fuse_model(self):\n",
        "        # fuse first layers\n",
        "        fuse_modules(self, ['conv1', 'bn1', 'relu'], inplace=True)\n",
        "        for m in self.modules():\n",
        "            if type(m) == QuantizableBottleneck or type(m) == QuantizableBasicBlock:\n",
        "                m.fuse_model()\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    model = QuantizableResNet(block, layers, **kwargs)\n",
        "\n",
        "    if pretrained:\n",
        "        model_url = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n",
        "        state_dict = load_state_dict_from_url(model_url,\n",
        "                                              progress=progress)\n",
        "\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet18', QuantizableBasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIIqEgKyps_V"
      },
      "source": [
        "#### Fusing\n",
        "Fusing is process to replace time-consuming operations with more faster but freezed or approximated. Currently we use fuze only on convolution and batchnorm layers.\n",
        "\n",
        "You can read more about it here:\n",
        "http://learnml.today/speeding-up-model-with-fusing-batch-normalization-and-convolution-3\n",
        "\n",
        "So what we do above is added `fuse_model` method to fuse layers and replaces residual operation with `nn.quantized.FloatFunctional()`.\n",
        "\n",
        "Also we set all `ReLU(inplace=False)`, this needed for quantizitaion module. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roZlNwumqcx4"
      },
      "source": [
        "Now let's try quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaRhNYbYIYpC"
      },
      "source": [
        "class MeanMetric(object):\n",
        "    \"\"\"Computes accuracy mean\"\"\"\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.clear()\n",
        "\n",
        "    def clear(self):\n",
        "        self.val = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val):\n",
        "        self.val += val\n",
        "        self.count += 1\n",
        "\n",
        "    def __str__(self):\n",
        "        return f'{self.name}={round(self.val/self.count, 2)}'\n",
        "\n",
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size = ('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')\n",
        "    return size\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "def evaluate(model, criterion, data_loader, device, eval_steps=10):\n",
        "    model.eval().to(device)\n",
        "    top1 = MeanMetric('Acc@1')\n",
        "    top5 = MeanMetric('Acc@5')\n",
        "    cnt = 0\n",
        "    with torch.no_grad():\n",
        "        for image, target in data_loader:\n",
        "            image, target = image.to(device), target.to(device)\n",
        "            output = model(image)\n",
        "            loss = criterion(output, target)\n",
        "            cnt += 1\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            top1.update(acc1[0].detach().cpu().item())\n",
        "            top5.update(acc5[0].detach().cpu().item())\n",
        "            if cnt >= eval_steps:\n",
        "                 return top1, top5\n",
        "\n",
        "    return top1, top5\n",
        "\n",
        "def load_model(model_file, classes=100):\n",
        "    model = resnet18(num_classes=classes)\n",
        "    state_dict = torch.load(model_file)\n",
        "    model.load_state_dict(state_dict)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSC5S6aGGAhU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fe3eb703-90b3-41a6-ad8e-a27e581d804b"
      },
      "source": [
        "def get_loaders(data_path, batch_size=8):\n",
        "\n",
        "    traindir = os.path.join(data_path, 'train')\n",
        "    valdir = os.path.join(data_path, 'val')\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    \n",
        "    # We want to say how we happy to know that in 21st cetury people still \n",
        "    # blocking other people from getting data to learn. That definitelly what \n",
        "    # will make our world better. Our thanks goes to the guys from ImageNet\n",
        "    # who locked down public access to the ImageNet Dataset.\n",
        "\n",
        "    dataset = torchvision.datasets.CIFAR100(\n",
        "        './cifar100',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "    )\n",
        "        \n",
        "    dataset_test = torchvision.datasets.CIFAR100(\n",
        "        './cifar100',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
        "    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size,\n",
        "        sampler=train_sampler)\n",
        "\n",
        "    data_loader_test = torch.utils.data.DataLoader(\n",
        "        dataset_test, batch_size=batch_size,\n",
        "        sampler=test_sampler)\n",
        "\n",
        "    return data_loader, data_loader_test\n",
        "\n",
        "train, test = get_loaders('imagenet_1k')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./cifar100/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "169009152it [00:04, 37868417.69it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./cifar100/cifar-100-python.tar.gz to ./cifar100\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wIHBohkIJB3"
      },
      "source": [
        "def train_one_epoch(model, criterion, optimizer, data_loader, device, log_steps=30):\n",
        "    model.train().to(device)\n",
        "    top1 = MeanMetric('Acc@1')\n",
        "    top5 = MeanMetric('Acc@5')\n",
        "    avgloss = MeanMetric('Loss')\n",
        "\n",
        "    cnt = 0\n",
        "    for image, target in data_loader:\n",
        "        start_time = time.time()\n",
        "        cnt += 1\n",
        "        \n",
        "        image, target = image.to(device), target.to(device)\n",
        "        output = model(image)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        top1.update(acc1[0].detach().cpu().item())\n",
        "        top5.update(acc5[0].detach().cpu().item())\n",
        "        avgloss.update(loss.detach().cpu().item())\n",
        "\n",
        "        if cnt % log_steps == 0 and cnt:\n",
        "            print(f'Training {cnt}: {avgloss} {top1} {top5}')\n",
        "            \n",
        "            avgloss.clear()\n",
        "            top1.clear()\n",
        "            top5.clear()\n",
        "\n",
        "                \n",
        "    top1, top5 = evaluate(model, criterion, data_loader, device)\n",
        "    print(f'Full imagenet train set: {top1} {top5}')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfVfbsWgpTuz"
      },
      "source": [
        "Let's train a bit our model on CIFAR100 dataset to get some basic accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvQwvtbOLr9R"
      },
      "source": [
        "model = resnet18(pretrained=False, num_classes=100).to('cuda')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=5e-4)\n",
        "\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'Epoch: {i} ######################################################')\n",
        "    model = train_one_epoch(model, criterion, optimizer, train, device='cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufRzfyF2ai2F"
      },
      "source": [
        "torch.save(model.state_dict(), 'resnet18.pth')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OprlOfE_eDkQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "38cbf4a3-66a2-4e4e-bc2c-4f92a8279abd"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "train, test = get_loaders('imagenet_1k')\n",
        "st = time.time()\n",
        "top1, top5 = evaluate(model, criterion, test, 'cpu')\n",
        "print(f'Finall accuracy: {top1} {top5} in {time.time() - st} sec')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b877bfc9be7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imagenet_1k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtop1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Finall accuracy: {top1} {top5} in {time.time() - st} sec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb-pnnSFendX"
      },
      "source": [
        "### Dynamic quantization\n",
        "Currently supported only for nn.Linear, nn.LSTM modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo2S9LTAeexg"
      },
      "source": [
        "base_model_size = print_size_of_model(model)\n",
        "print(base_model_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HitthGIeE2P-"
      },
      "source": [
        "import torch.quantization\n",
        "\n",
        "base = load_model('resnet18.pth').eval().to('cpu')\n",
        "# we will se DynamicQuantizedLinear module in the end instead of Linear\n",
        "dyn_quantized_model = torch.quantization.quantize_dynamic(base, {nn.Linear}, dtype=torch.qint8)\n",
        "print(list(dyn_quantized_model.modules())[-3:])\n",
        "\n",
        "base_model_size = print_size_of_model(dyn_quantized_model)\n",
        "print(base_model_size)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "train, test = get_loaders('imagenet_1k')\n",
        "st = time.time()\n",
        "top1, top5 = evaluate(dyn_quantized_model, criterion, test, 'cpu')\n",
        "print(f'Finall accuracy: {top1} {top5} in {time.time() - st} sec')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_yB0RlFikoc"
      },
      "source": [
        "### Static quantization\n",
        "\n",
        "Quantization is based on Observer class, which main purpose is to find best parameters to fit float32 into int8. So to give some info about weights and biases distribution we need to inference some amount of data through the model. We do this after the `prepare` method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nktMZjteiz_G"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "stat_quant = load_model('resnet18.pth').eval().to('cpu')\n",
        "\n",
        "# Fuse Conv, bn and relu\n",
        "stat_quant.fuse_model()\n",
        "# we will not see Batch Norm instead will see replaced it with Identity\n",
        "print(stat_quant)\n",
        "\n",
        "stat_quant.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "print(stat_quant.qconfig)\n",
        "\n",
        "# this aware quantization engine thet we are going to inference few inputs \n",
        "# so Observer can get enough data for building optimiation maps\n",
        "torch.quantization.prepare(stat_quant, inplace=True)\n",
        "train, test = get_loaders('imagenet_1k')\n",
        "evaluate(stat_quant, criterion, test, 'cpu')\n",
        "# now we converting our model to int8\n",
        "torch.quantization.convert(stat_quant, inplace=True)\n",
        "\n",
        "base_model_size = print_size_of_model(stat_quant)\n",
        "print(base_model_size)\n",
        "\n",
        "st = time.time()\n",
        "top1, top5 = evaluate(stat_quant, criterion, test, 'cpu')\n",
        "print(f'Finall accuracy: {top1} {top5} in {time.time() - st} sec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGS2AkbTj6Cz"
      },
      "source": [
        "So we have small accuracy drop, but we lower size of our model in 4 times and inference times droped in twice. Not bad for few lines of code :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QGFjBExkSji"
      },
      "source": [
        "## Quantization-aware training\n",
        "So this works preatty simple. QAT converts all weights and activations to *fake quantized*, so during forward and backward passes float values are rounded to mimic int8 values. Thus during training we optimize values of weights and activations so they better fit to the int8 values distribution. And after training we just fix that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THZlaer9ji__"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "qat_model = load_model('resnet18.pth')\n",
        "optimizer = torch.optim.SGD(qat_model.parameters(), momentum=0.9, lr=5e-4)\n",
        "qat_model.fuse_model()\n",
        "\n",
        "qat_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
        "torch.quantization.prepare_qat(qat_model, inplace=True)\n",
        "\n",
        "train, test = get_loaders('imagenet_1k')\n",
        "# Train and check accuracy after each epoch\n",
        "for epoch in range(5):\n",
        "    train_one_epoch(qat_model, criterion, optimizer, train, 'cuda')\n",
        "    if epoch > 3:\n",
        "        # Freeze quantizer parameters\n",
        "        qat_model.apply(torch.quantization.disable_observer)\n",
        "    if epoch > 2:\n",
        "        # Freeze batch norm mean and variance estimates\n",
        "        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
        "\n",
        "    # Check the accuracy after each epoch\n",
        "    quantized_model = torch.quantization.convert(qat_model.eval().to('cpu'), inplace=False)\n",
        "    quantized_model.eval()\n",
        "    top1, top5 = evaluate(quantized_model, criterion, test, 'cpu')\n",
        "    print(f'Epoch {epoch}: {top1} {top5}')\n",
        "\n",
        "base_model_size = print_size_of_model(stat_quant)\n",
        "print(base_model_size)\n",
        "\n",
        "st = time.time()\n",
        "top1, top5 = evaluate(stat_quant, criterion, test, 'cpu')\n",
        "print(f'Finall accuracy: {top1} {top5} in {time.time() - st} sec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG4N9XNe6QPE"
      },
      "source": [
        "## Nvidia APEX\n",
        "\n",
        "This lib is a quite cool stuff to get additional boost just with few lines, or more wih more advanced techniques.\n",
        "What it do is same Quantized Aware Training. It give you ability to use mixed float precision with float32/float16, or fully float16 precision.\n",
        "\n",
        "This is usefull because many edge device have much better float16 support than float32, also new Nvidia cards have preatty good optimization on float16. Here is example of Nvidia T4 card:\n",
        "\n",
        "<img src='https://github.com/learnml-today/object-detection-with-pytorch/blob/master/imgs/nvidiat4.png?raw=true' />\n",
        "\n",
        "As you see difference betwen float32 and float16 is more than 8x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJfyA3TD7ipH"
      },
      "source": [
        "try:\n",
        "    from apex import amp\n",
        "except ImportError:\n",
        "    raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to run this example.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-IhJtcbK1dq"
      },
      "source": [
        "def train_one_epoch_with_apex(model, criterion, optimizer, data_loader, device, log_steps=30):\n",
        "    model.train().to(device)\n",
        "    top1 = MeanMetric('Acc@1')\n",
        "    top5 = MeanMetric('Acc@5')\n",
        "    avgloss = MeanMetric('Loss')\n",
        "\n",
        "    cnt = 0\n",
        "    for image, target in data_loader:\n",
        "        start_time = time.time()\n",
        "        cnt += 1\n",
        "        \n",
        "        image, target = image.to(device), target.to(device)\n",
        "        output = model(image)\n",
        "        loss = criterion(output, target)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        with amp.scale_loss(loss, optimizer) as scaled_loss:           \n",
        "            scaled_loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        top1.update(acc1[0].detach().cpu().item())\n",
        "        top5.update(acc5[0].detach().cpu().item())\n",
        "        avgloss.update(loss.detach().cpu().item())\n",
        "\n",
        "        if cnt % log_steps == 0 and cnt:\n",
        "            print(f'Training {cnt}: {avgloss} {top1} {top5}')\n",
        "            \n",
        "            avgloss.clear()\n",
        "            top1.clear()\n",
        "            top5.clear()\n",
        "\n",
        "                \n",
        "    top1, top5 = evaluate(model, criterion, data_loader, device)\n",
        "    print(f'Full imagenet train set: {top1} {top5}')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjfXUEYd6ZfL"
      },
      "source": [
        "opt_level = 'O1' # mixed float32 and float16 precision\n",
        "model = resnet18(pretrained=False, num_classes=100).to('cuda')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=5e-4)\n",
        "# Loss scaling here can be used to preserve small gradient values. if not set used dynamic\n",
        "model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\n",
        "\n",
        "st = time.time()\n",
        "for i in range(1):\n",
        "    print(f'Epoch: {i} ######################################################')\n",
        "    model = train_one_epoch_with_apex(model, criterion, optimizer, train, device='cuda')\n",
        "print('Total time:', time.time()-st)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWCSId4t8Op7"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "train, test = get_loaders('imagenet_1k')\n",
        "st = time.time()\n",
        "top1, top5 = evaluate(model, criterion, test, 'cuda')\n",
        "print(f'Finall accuracy: {top1} {top5} in {time.time() - st} sec')\n",
        "\n",
        "base_model_size = print_size_of_model(model)\n",
        "print(base_model_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buYIkeSIoz8r"
      },
      "source": [
        "So as we see size not changes much, but the inference speed becomes faster."
      ]
    }
  ]
}