{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "601b4181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:00:05.587280Z",
     "iopub.status.busy": "2022-07-05T07:00:05.586556Z",
     "iopub.status.idle": "2022-07-05T07:01:05.997986Z",
     "shell.execute_reply": "2022-07-05T07:01:05.997021Z"
    },
    "papermill": {
     "duration": 60.432141,
     "end_time": "2022-07-05T07:01:06.008615",
     "exception": false,
     "start_time": "2022-07-05T07:00:05.576474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./sentence-transformers\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=124483 sha256=c0d7a796f2323694525dcd9f1f2c70df9f32c2d364d3bf5477ba7148203117a4\n",
      "  Stored in directory: /root/.cache/pip/wheels/2a/dd/1f/d5e3ed645ab6e4c3bf10ce5be36e121cf4289cb69861525509\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# install sentence transformers from dataset\n",
    "cp -r ../input/sentencetransformers-sourceandsomemodels/sentence-transformers ./sentence-transformers\n",
    "pip install -U --no-build-isolation --no-deps ./sentence-transformers\n",
    "\n",
    "## copy sentence transformers pretrained models and configuration files from dataset to local caches\n",
    "mkdir -p  /root/.cache/torch/\n",
    "\n",
    "cp -r ../input/sentencetransformers-sourceandsomemodels/torch/sentence_transformers /root/.cache/torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eefac7a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:01:06.025735Z",
     "iopub.status.busy": "2022-07-05T07:01:06.025434Z",
     "iopub.status.idle": "2022-07-05T07:01:19.165064Z",
     "shell.execute_reply": "2022-07-05T07:01:19.164083Z"
    },
    "papermill": {
     "duration": 13.15018,
     "end_time": "2022-07-05T07:01:19.167081",
     "exception": false,
     "start_time": "2022-07-05T07:01:06.016901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ../input/pykakasi/pykakasi.deps\r\n",
      "   creating: ./pykakasi/\r\n",
      "  inflating: ./pykakasi/zipp-3.8.0-py3-none-any.whl  \r\n",
      "  inflating: ./pykakasi/jaconv-0.3.tar.gz  \r\n",
      "  inflating: ./pykakasi/wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl  \r\n",
      "  inflating: ./pykakasi/typing_extensions-4.2.0-py3-none-any.whl  \r\n",
      "  inflating: ./pykakasi/importlib_metadata-4.11.4-py3-none-any.whl  \r\n",
      "  inflating: ./pykakasi/pykakasi-2.2.1-py3-none-any.whl  \r\n",
      "  inflating: ./pykakasi/Deprecated-1.2.13-py2.py3-none-any.whl  \r\n",
      "Looking in links: ./pykakasi\r\n",
      "Processing ./pykakasi/pykakasi-2.2.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from pykakasi) (4.11.4)\r\n",
      "Processing ./pykakasi/jaconv-0.3.tar.gz\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hProcessing ./pykakasi/Deprecated-1.2.13-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from deprecated->pykakasi) (1.14.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->pykakasi) (4.2.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->pykakasi) (3.8.0)\r\n",
      "Building wheels for collected packages: jaconv\r\n",
      "  Building wheel for jaconv (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for jaconv: filename=jaconv-0.3-py3-none-any.whl size=15564 sha256=4b12584369fa1d9cd5663a9c368978716985759a53378f9ee96b1d5038a58520\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/44/18/e8/e2dd6364fca3327d0d307eaea165d28d84e139fcc6200853b0\r\n",
      "Successfully built jaconv\r\n",
      "Installing collected packages: jaconv, deprecated, pykakasi\r\n",
      "Successfully installed deprecated-1.2.13 jaconv-0.3 pykakasi-2.2.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!unzip  ../input/pykakasi/pykakasi.deps -d ./\n",
    "!pip install --no-index --find-links=./pykakasi pykakasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb0bb381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:01:19.185501Z",
     "iopub.status.busy": "2022-07-05T07:01:19.184749Z",
     "iopub.status.idle": "2022-07-05T07:01:19.191531Z",
     "shell.execute_reply": "2022-07-05T07:01:19.190816Z"
    },
    "papermill": {
     "duration": 0.017673,
     "end_time": "2022-07-05T07:01:19.193273",
     "exception": false,
     "start_time": "2022-07-05T07:01:19.175600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "VERBOSE = False\n",
    "\n",
    "def display_data():\n",
    "    if VERBOSE:\n",
    "        data = pd.read_feather(\"data.feather\")\n",
    "        display(data.head())\n",
    "\n",
    "def display_pairs():\n",
    "    if VERBOSE:\n",
    "        pairs = pd.read_feather(\"pairs.feather\")\n",
    "        display(pairs.head())\n",
    "def display_features():\n",
    "    if VERBOSE:\n",
    "        pairs = pd.read_feather(\"pairs.feather\")\n",
    "        cols = [c for c in pairs.columns]\n",
    "        print(len(cols))\n",
    "        display(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d9304b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:01:19.211098Z",
     "iopub.status.busy": "2022-07-05T07:01:19.210415Z",
     "iopub.status.idle": "2022-07-05T07:01:19.216992Z",
     "shell.execute_reply": "2022-07-05T07:01:19.215800Z"
    },
    "papermill": {
     "duration": 0.017899,
     "end_time": "2022-07-05T07:01:19.219162",
     "exception": false,
     "start_time": "2022-07-05T07:01:19.201263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "import pandas as pd\n",
    "import sys\n",
    "def main():\n",
    "    args = sys.argv[1:]\n",
    "    if args[0] == \"test\":\n",
    "        df = pd.read_csv(\"../input/foursquare-location-matching/test.csv\")\n",
    "        cols = [ c for c in df.columns]\n",
    "        if df.shape[0] == 5:\n",
    "            df = pd.read_csv(\"../input/foursquare-location-matching/train.csv\")\n",
    "            df = df[cols][:100]\n",
    "    elif args[0] == \"train\":\n",
    "        df = pd.read_feather(\"../input/flm-kfold-pairs/train_data.feather\")\n",
    "        if DEBUG:\n",
    "            df = df[:100]\n",
    "    elif args[0] == \"valid\":\n",
    "        df = pd.read_feather(\"../input/flm-kfold-pairs/valid_data.feather\")\n",
    "        if DEBUG:\n",
    "            df = df[:100]        \n",
    "    \n",
    "    print (f\"{args[0]} data shape:{df.shape}\")\n",
    "    cols = [c for c in df.columns]\n",
    "    print (cols)\n",
    "    df.to_feather (\"data.feather\")\n",
    "    \n",
    "DEBUG = False\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df2725c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:01:19.237311Z",
     "iopub.status.busy": "2022-07-05T07:01:19.236786Z",
     "iopub.status.idle": "2022-07-05T07:01:28.724155Z",
     "shell.execute_reply": "2022-07-05T07:01:28.723032Z"
    },
    "papermill": {
     "duration": 9.498115,
     "end_time": "2022-07-05T07:01:28.726129",
     "exception": false,
     "start_time": "2022-07-05T07:01:19.228014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data shape:(100, 12)\r\n",
      "['id', 'name', 'latitude', 'longitude', 'address', 'city', 'state', 'zip', 'country', 'url', 'phone', 'categories']\r\n",
      "CPU times: user 94.2 ms, sys: 25.8 ms, total: 120 ms\n",
      "Wall time: 9.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python setup.py test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e29edce",
   "metadata": {
    "papermill": {
     "duration": 0.008314,
     "end_time": "2022-07-05T07:01:28.743634",
     "exception": false,
     "start_time": "2022-07-05T07:01:28.735320",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Text2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2615c5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:01:28.761481Z",
     "iopub.status.busy": "2022-07-05T07:01:28.761180Z",
     "iopub.status.idle": "2022-07-05T07:01:28.767440Z",
     "shell.execute_reply": "2022-07-05T07:01:28.766664Z"
    },
    "papermill": {
     "duration": 0.018505,
     "end_time": "2022-07-05T07:01:28.770357",
     "exception": false,
     "start_time": "2022-07-05T07:01:28.751852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing text2vec.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile text2vec.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "def sentence_transformer_feat2Vec (model_name,df, feat): \n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "    text_df = df[[feat]].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    vec =  model.encode(text_df[feat].values, show_progress_bar=True)\n",
    "\n",
    "    return  text_df, vec                    \n",
    "\n",
    "def main():\n",
    "\n",
    "\n",
    "    df = pd.read_feather(\"data.feather\")\n",
    "\n",
    "    df[\"name\"] =  df[\"name\"].fillna(\"unknow\")\n",
    "    df[\"address\"] =  df[\"address\"].fillna(\"unknow\")\n",
    "    df[\"city\"] =  df[\"city\"].fillna(\"unknow\")\n",
    "    df[\"state\"] =  df[\"state\"].fillna(\"unknow\")\n",
    "    df[\"country\"] =  df[\"country\"].fillna(\"unknow\")\n",
    "    df[\"categories\"] =  df[\"categories\"].fillna(\"unknow\")\n",
    "    #df[\"city_state_country\"] = df[\"city\"]+ \", \" +  df[\"state\"] + \", \" + df[\"country\"]\n",
    "\n",
    "\n",
    "    for model_name in [\n",
    "        'sentence-transformers_paraphrase-xlm-r-multilingual-v1', \n",
    "        'sentence-transformers_paraphrase-multilingual-mpnet-base-v2',\n",
    "        'sentence-transformers_all-mpnet-base-v2',\n",
    "    ]:\n",
    "\n",
    "        for feat in [ \"categories\", \"city\", \"state\", \"name\", \"address\" ]:\n",
    "\n",
    "            print(f\"{model_name}: {feat}\")\n",
    "            text_df, vec = sentence_transformer_feat2Vec (f\"{LOCAL_CACHE}sentence_transformers/{model_name}\",   df, feat)\n",
    "            text_df.to_csv(f\"{model_name}_{feat}.csv\", index=False)\n",
    "\n",
    "            with open(f'{model_name}_{feat}.vec', 'wb') as handle:\n",
    "                pickle.dump(vec, handle)\n",
    "            print(f\"vec:{vec.shape}\")\n",
    "\n",
    "            \n",
    "LOCAL_CACHE = '../input/sentencetransformers-sourceandsomemodels/torch/'\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8c92a65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:01:28.789516Z",
     "iopub.status.busy": "2022-07-05T07:01:28.789257Z",
     "iopub.status.idle": "2022-07-05T07:02:42.200368Z",
     "shell.execute_reply": "2022-07-05T07:02:42.199169Z"
    },
    "papermill": {
     "duration": 73.422212,
     "end_time": "2022-07-05T07:02:42.202412",
     "exception": false,
     "start_time": "2022-07-05T07:01:28.780200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-transformers_paraphrase-xlm-r-multilingual-v1: categories\r\n",
      "Batches: 100%|████████████████████████████████████| 3/3 [00:00<00:00,  3.11it/s]\r\n",
      "vec:(82, 768)\r\n",
      "sentence-transformers_paraphrase-xlm-r-multilingual-v1: city\r\n",
      "Batches: 100%|████████████████████████████████████| 3/3 [00:00<00:00, 91.28it/s]\r\n",
      "vec:(77, 768)\r\n",
      "sentence-transformers_paraphrase-xlm-r-multilingual-v1: state\r\n",
      "Batches: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 61.72it/s]\r\n",
      "vec:(57, 768)\r\n",
      "sentence-transformers_paraphrase-xlm-r-multilingual-v1: name\r\n",
      "Batches: 100%|████████████████████████████████████| 4/4 [00:00<00:00, 58.84it/s]\r\n",
      "vec:(99, 768)\r\n",
      "sentence-transformers_paraphrase-xlm-r-multilingual-v1: address\r\n",
      "Batches: 100%|████████████████████████████████████| 3/3 [00:00<00:00, 65.79it/s]\r\n",
      "vec:(71, 768)\r\n",
      "sentence-transformers_paraphrase-multilingual-mpnet-base-v2: categories\r\n",
      "Batches: 100%|████████████████████████████████████| 3/3 [00:00<00:00, 74.49it/s]\r\n",
      "vec:(82, 768)\r\n",
      "sentence-transformers_paraphrase-multilingual-mpnet-base-v2: city\r\n",
      "Batches: 100%|████████████████████████████████████| 3/3 [00:00<00:00, 87.44it/s]\r\n",
      "vec:(77, 768)\r\n",
      "sentence-transformers_paraphrase-multilingual-mpnet-base-v2: state\r\n",
      "Batches: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 66.24it/s]\r\n",
      "vec:(57, 768)\r\n",
      "sentence-transformers_paraphrase-multilingual-mpnet-base-v2: name\r\n",
      "Batches: 100%|████████████████████████████████████| 4/4 [00:00<00:00, 60.46it/s]\r\n",
      "vec:(99, 768)\r\n",
      "sentence-transformers_paraphrase-multilingual-mpnet-base-v2: address\r\n",
      "Batches: 100%|████████████████████████████████████| 3/3 [00:00<00:00, 65.98it/s]\r\n",
      "vec:(71, 768)\r\n",
      "sentence-transformers_all-mpnet-base-v2: categories\r\n",
      "Batches: 100%|████████████████████████████████████| 3/3 [00:00<00:00, 37.39it/s]\r\n",
      "vec:(82, 768)\r\n",
      "sentence-transformers_all-mpnet-base-v2: city\r\n",
      "Batches: 100%|████████████████████████████████████| 3/3 [00:00<00:00, 68.87it/s]\r\n",
      "vec:(77, 768)\r\n",
      "sentence-transformers_all-mpnet-base-v2: state\r\n",
      "Batches: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 79.70it/s]\r\n",
      "vec:(57, 768)\r\n",
      "sentence-transformers_all-mpnet-base-v2: name\r\n",
      "Batches: 100%|████████████████████████████████████| 4/4 [00:00<00:00, 52.39it/s]\r\n",
      "vec:(99, 768)\r\n",
      "sentence-transformers_all-mpnet-base-v2: address\r\n",
      "Batches: 100%|████████████████████████████████████| 3/3 [00:00<00:00, 42.61it/s]\r\n",
      "vec:(71, 768)\r\n",
      "CPU times: user 812 ms, sys: 136 ms, total: 948 ms\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python text2vec.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01990bf",
   "metadata": {
    "papermill": {
     "duration": 0.010884,
     "end_time": "2022-07-05T07:02:42.224536",
     "exception": false,
     "start_time": "2022-07-05T07:02:42.213652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LatLong2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34372c4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:02:42.248177Z",
     "iopub.status.busy": "2022-07-05T07:02:42.247856Z",
     "iopub.status.idle": "2022-07-05T07:02:42.254247Z",
     "shell.execute_reply": "2022-07-05T07:02:42.253364Z"
    },
    "papermill": {
     "duration": 0.019858,
     "end_time": "2022-07-05T07:02:42.255912",
     "exception": false,
     "start_time": "2022-07-05T07:02:42.236054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing latlong2vec.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile latlong2vec.py\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pickle\n",
    "\n",
    "\n",
    "#https://stackoverflow.com/questions/10473852/convert-latitude-and-longitude-to-point-in-3d-space\n",
    "def LLHtoECEF(lat, lon):\n",
    "    rad = np.float64(6378137.0)        # Radius of the Earth (in meters)\n",
    "    f = np.float64(1.0/298.257223563)  # Flattening factor WGS84 Model\n",
    "    LLHtoECEF_FF = (1.0-f)**2\n",
    "\n",
    "    cosLat = np.cos(lat)\n",
    "    sinLat = np.sin(lat)\n",
    "    C = 1/np.sqrt(cosLat**2 + LLHtoECEF_FF * sinLat**2)\n",
    "    S = C * LLHtoECEF_FF\n",
    "\n",
    "    x = (rad * C)*cosLat * np.cos(lon)\n",
    "    y = (rad * C)*cosLat * np.sin(lon)\n",
    "    z = (rad * S)*sinLat\n",
    "    \n",
    "\n",
    "    mat = np.vstack((x,y,z)).T\n",
    "\n",
    "    mat = mat / np.linalg.norm(mat, axis=1).reshape((-1, 1))\n",
    "\n",
    "    \n",
    "    return mat\n",
    "\n",
    "def lat_lon_feat2vec(df):\n",
    "    lat_lon_matrix = df[[\"latitude\",\"longitude\"]].values\n",
    "    vec = LLHtoECEF ( lat_lon_matrix[:,0], lat_lon_matrix[:,1] )\n",
    "    \n",
    "    return vec\n",
    "\n",
    "def main():\n",
    "    data = pd.read_feather(\"data.feather\")\n",
    "    vec = lat_lon_feat2vec(data)\n",
    "    with open(f'lat_lon.vec', 'wb') as handle:\n",
    "        pickle.dump(vec, handle)\n",
    "\n",
    "    print(f\"latlong2vec shape:{vec.shape}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04ea1434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:02:42.278931Z",
     "iopub.status.busy": "2022-07-05T07:02:42.278390Z",
     "iopub.status.idle": "2022-07-05T07:02:43.713450Z",
     "shell.execute_reply": "2022-07-05T07:02:43.712043Z"
    },
    "papermill": {
     "duration": 1.448689,
     "end_time": "2022-07-05T07:02:43.715502",
     "exception": false,
     "start_time": "2022-07-05T07:02:42.266813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latlong2vec shape:(100, 3)\r\n",
      "CPU times: user 11.4 ms, sys: 5.24 ms, total: 16.7 ms\n",
      "Wall time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python latlong2vec.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1b4f00",
   "metadata": {
    "papermill": {
     "duration": 0.011004,
     "end_time": "2022-07-05T07:02:43.738852",
     "exception": false,
     "start_time": "2022-07-05T07:02:43.727848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate pairs (pykakasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55e25138",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:02:43.762939Z",
     "iopub.status.busy": "2022-07-05T07:02:43.762621Z",
     "iopub.status.idle": "2022-07-05T07:02:43.769072Z",
     "shell.execute_reply": "2022-07-05T07:02:43.768322Z"
    },
    "papermill": {
     "duration": 0.020669,
     "end_time": "2022-07-05T07:02:43.771045",
     "exception": false,
     "start_time": "2022-07-05T07:02:43.750376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing convert.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile convert.py\n",
    "import pandas as pd\n",
    "import pykakasi\n",
    "\n",
    "\n",
    "def convert_japanese_alphabet(df: pd.DataFrame):\n",
    "    kakasi = pykakasi.kakasi()\n",
    "    kakasi.setMode('H', 'a')  # Convert Hiragana into alphabet\n",
    "    kakasi.setMode('K', 'a')  # Convert Katakana into alphabet\n",
    "    kakasi.setMode('J', 'a')  # Convert Kanji into alphabet\n",
    "    conversion = kakasi.getConverter()\n",
    "\n",
    "    def convert(row):\n",
    "        for column in [\"name\", \"address\", \"city\", \"state\"]:\n",
    "            try:\n",
    "                row[column] = conversion.do(row[column])\n",
    "            except:\n",
    "                pass\n",
    "        return row\n",
    "\n",
    "    df[df[\"country\"] == \"JP\"] = df[df[\"country\"] == \"JP\"].apply(convert, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = pd.read_feather(\"data.feather\")\n",
    "    print(data.query(\"country == 'JP'\")[\"name\"].values[:10])\n",
    "    \n",
    "    data = data [[\"id\", \"name\", \"address\", \"city\", \"state\", \"country\"]]\n",
    "    print(f\"shape:{data.shape}\")\n",
    "\n",
    "    data = convert_japanese_alphabet(data)\n",
    "    print(data.query(\"country == 'JP'\")[\"name\"].values[:10])    \n",
    "    print(f\"shape:{data.shape}\")\n",
    "    \n",
    "    data.to_feather(\"convert_data.feather\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57cbf2a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:02:43.794948Z",
     "iopub.status.busy": "2022-07-05T07:02:43.794384Z",
     "iopub.status.idle": "2022-07-05T07:02:45.838024Z",
     "shell.execute_reply": "2022-07-05T07:02:45.836891Z"
    },
    "papermill": {
     "duration": 2.057654,
     "end_time": "2022-07-05T07:02:45.839909",
     "exception": false,
     "start_time": "2022-07-05T07:02:43.782255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['つじ田 味噌の章' 'ローソン 弘前撫牛子店' 'ざま駅前歯科医院' '下高井戸 どどん' '虎萬元 南青山店'\r\n",
      " 'イオン乙金ショッピングセンター' '甲武信小屋']\r\n",
      "shape:(100, 6)\r\n",
      "convert.py:7: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\r\n",
      "  kakasi.setMode('H', 'a')  # Convert Hiragana into alphabet\r\n",
      "convert.py:8: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\r\n",
      "  kakasi.setMode('K', 'a')  # Convert Katakana into alphabet\r\n",
      "convert.py:9: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\r\n",
      "  kakasi.setMode('J', 'a')  # Convert Kanji into alphabet\r\n",
      "convert.py:10: DeprecationWarning: Call to deprecated method getConverter. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\r\n",
      "  conversion = kakasi.getConverter()\r\n",
      "convert.py:15: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\r\n",
      "  row[column] = conversion.do(row[column])\r\n",
      "convert.py:20: SettingWithCopyWarning: \r\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\r\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\r\n",
      "\r\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\r\n",
      "  df[df[\"country\"] == \"JP\"] = df[df[\"country\"] == \"JP\"].apply(convert, axis=1)\r\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \r\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\r\n",
      "\r\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\r\n",
      "  self._setitem_single_block(indexer, value, name)\r\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:723: SettingWithCopyWarning: \r\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\r\n",
      "\r\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\r\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\r\n",
      "['tsujita misonoshou' 'rooson hirosakibuushikomise' 'zamaekimaeshikaiin'\r\n",
      " 'shimotakaido dodon' 'toramanmoto minamiaoyamamise'\r\n",
      " 'ionotsukinshoppingusentaa' 'kobushikoya']\r\n",
      "shape:(100, 6)\r\n",
      "CPU times: user 22.4 ms, sys: 4.68 ms, total: 27.1 ms\n",
      "Wall time: 2.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python convert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1faf9854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:02:45.866545Z",
     "iopub.status.busy": "2022-07-05T07:02:45.865734Z",
     "iopub.status.idle": "2022-07-05T07:02:45.881952Z",
     "shell.execute_reply": "2022-07-05T07:02:45.880953Z"
    },
    "papermill": {
     "duration": 0.031511,
     "end_time": "2022-07-05T07:02:45.883634",
     "exception": false,
     "start_time": "2022-07-05T07:02:45.852123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing generate_pairs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generate_pairs.py\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    import cuml\n",
    "    from cuml.neighbors import NearestNeighbors \n",
    "    print(f\"cuml:{cuml.__version__}\")\n",
    "else:\n",
    "    from sklearn.neighbors import NearestNeighbors \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer    \n",
    "    \n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)    \n",
    "    \n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#https://stackoverflow.com/questions/10473852/convert-latitude-and-longitude-to-point-in-3d-space\n",
    "def LLHtoECEF(lat, lon):\n",
    "    rad = np.float64(6378137.0)        # Radius of the Earth (in meters)\n",
    "    f = np.float64(1.0/298.257223563)  # Flattening factor WGS84 Model\n",
    "    LLHtoECEF_FF = (1.0-f)**2\n",
    "\n",
    "    cosLat = np.cos(lat)\n",
    "    sinLat = np.sin(lat)\n",
    "    C = 1/np.sqrt(cosLat**2 + LLHtoECEF_FF * sinLat**2)\n",
    "    S = C * LLHtoECEF_FF\n",
    "\n",
    "    x = (rad * C)*cosLat * np.cos(lon)\n",
    "    y = (rad * C)*cosLat * np.sin(lon)\n",
    "    z = (rad * S)*sinLat\n",
    "    \n",
    "\n",
    "    mat = np.vstack((x,y,z)).T\n",
    "\n",
    "    mat = mat / np.linalg.norm(mat, axis=1).reshape((-1, 1))\n",
    "\n",
    "    \n",
    "    return mat\n",
    "\n",
    "def lat_lon_feat2vec(df):\n",
    "    lat_lon_matrix = df[[\"latitude\",\"longitude\"]].values\n",
    "    vec = LLHtoECEF ( lat_lon_matrix[:,0], lat_lon_matrix[:,1] )\n",
    "    \n",
    "    return vec\n",
    "\n",
    "\n",
    "def recall_knn_latlong (df, neighbors, total_neighbors, threshold  ):\n",
    "\n",
    "\n",
    "    \n",
    "    pairs = []\n",
    "    for country, country_df in tqdm(df.groupby('country')):\n",
    "        pairs_country = []\n",
    "        size = country_df.shape[0] \n",
    "        vec = lat_lon_feat2vec(country_df)\n",
    "        \n",
    "\n",
    " \n",
    "        total_neighbors_ = min ( size, total_neighbors)\n",
    "        neighbors_ = min(size, neighbors)\n",
    "   \n",
    "        if total_neighbors_ > 1:        \n",
    "            knn = NearestNeighbors(n_neighbors = total_neighbors_, metric = 'cosine',  algorithm=\"brute\", n_jobs = -1)\n",
    "          \n",
    "            knn.fit(vec)\n",
    "            dists, nears = knn.kneighbors(vec, return_distance = True)    \n",
    "            mean_1 = dists[:, :neighbors_].mean(axis=1)\n",
    "            mean_2 = dists[:, :total_neighbors_].mean(axis=1)\n",
    "            for k in range(0,total_neighbors_):            \n",
    "                cur_df = country_df[['id']]\n",
    "                cur_df['match_id'] = country_df['id'].values[nears[:, k]]\n",
    "                cur_df[f'kdist'] = dists[:, k]\n",
    "                cur_df[f'kneighbors'] = k\n",
    "                cur_df[f'kdist_mean_1'] = mean_1\n",
    "                cur_df[f'kdist_mean_2'] = mean_2\n",
    "                cur_df = cur_df.query(\"id != match_id  and (kneighbors < @neighbors_ or kdist < @threshold) \")\n",
    "\n",
    "                if len(cur_df) > 0:\n",
    "\n",
    "                    flag = cur_df[\"id\"] > cur_df[\"match_id\"]\n",
    "                    ids = cur_df[flag][\"id\"].values\n",
    "                    match_ids = cur_df[flag][\"match_id\"].values\n",
    "\n",
    "                    cur_df.loc[flag, \"id\"] = match_ids\n",
    "                    cur_df.loc[flag, \"match_id\"] = ids\n",
    "                    pairs_country.append(cur_df)\n",
    "                    \n",
    "            pairs_country = pd.concat (pairs_country)\n",
    "                    \n",
    "            pairs_country = pairs_country.groupby ( [\"id\",\"match_id\"]).agg ( {\n",
    "                                                                f'kneighbors':[\"mean\"],\n",
    "                                                                f'kdist_mean_1': [\"mean\"],\n",
    "                                                                f'kdist_mean_2': [\"mean\"],\n",
    "                                                                f'kdist':[\"count\"]\n",
    "                                                                   } ).reset_index() \n",
    "\n",
    "            pairs_country.columns =  ['_'.join(col) for col in pairs_country.columns.values]\n",
    "            pairs_country = pairs_country.rename(columns={\"id_\":\"id\", \"match_id_\":\"match_id\"})\n",
    "            pairs.append(pairs_country)\n",
    "\n",
    "\n",
    "    pairs = pd.concat(pairs)\n",
    "    \n",
    "    print(f\"recall_knn_latlong: {pairs.shape}\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "def recall_knn_vec ( df, neighbors, total_neighbors, threshold, vec_name ):\n",
    "\n",
    "    \n",
    "    name = pd.read_csv(f\"{TEXT2VEC_PATH}/{TEXT2VEC_PREFIX}{vec_name}.csv\")\n",
    "    with open(f'{TEXT2VEC_PATH}/{TEXT2VEC_PREFIX}{vec_name}.vec', 'rb') as handle:\n",
    "        name_vec = pickle.load(handle)\n",
    "    \n",
    "    d = name[vec_name].to_dict()\n",
    "    name2ids = { d[k]:k for k in d  }        \n",
    "    pairs = []\n",
    "    for country, country_df in tqdm(df.groupby('country')):\n",
    "        pairs_country = []\n",
    "        country_df = country_df[ country_df[vec_name].notnull() ]\n",
    "        names = country_df[vec_name].values\n",
    "        size = names.shape[0] \n",
    "        vec = np.zeros ((size, name_vec.shape[1]))\n",
    "        for i in range (size):\n",
    "            vec[i] = name_vec[ name2ids [names[i]] ,:] \n",
    "\n",
    "        total_neighbors_ = min ( size, total_neighbors)\n",
    "        neighbors_ = min(size, neighbors)\n",
    "        \n",
    "        if total_neighbors_ > 1:        \n",
    "            knn = NearestNeighbors(n_neighbors = total_neighbors_, metric = 'cosine',  algorithm=\"brute\", n_jobs = -1)\n",
    "            \n",
    "            #print (country, size, total_neighbors_, neighbors_)\n",
    "            \n",
    "            knn.fit(vec)\n",
    "            dists, nears = knn.kneighbors(vec, return_distance = True)    \n",
    "            mean_1 = dists[:, :neighbors_].mean(axis=1)\n",
    "            mean_2 = dists[:, :total_neighbors_].mean(axis=1)\n",
    "            \n",
    "            for k in range(0,total_neighbors_):            \n",
    "                cur_df = country_df[['id']]\n",
    "                cur_df['match_id'] = country_df['id'].values[nears[:, k]]\n",
    "                cur_df[f'xml_{vec_name}_kdist'] = dists[:, k]\n",
    "                cur_df[f'xml_{vec_name}_kneighbors'] = k\n",
    "                cur_df[f'xml_{vec_name}_kdist_mean_1'] = mean_1\n",
    "                cur_df[f'xml_{vec_name}_kdist_mean_2'] = mean_2\n",
    "                cur_df = cur_df.query(f\"id != match_id  and (xml_{vec_name}_kneighbors < @neighbors_ or xml_{vec_name}_kdist < @threshold) \")\n",
    "                if len(cur_df) > 0:\n",
    "                    flag = cur_df[\"id\"] > cur_df[\"match_id\"]\n",
    "                    ids = cur_df[flag][\"id\"].values\n",
    "                    match_ids = cur_df[flag][\"match_id\"].values\n",
    "\n",
    "                    cur_df.loc[flag, \"id\"] = match_ids\n",
    "                    cur_df.loc[flag, \"match_id\"] = ids\n",
    "                    pairs_country.append(cur_df)\n",
    "                    \n",
    "            pairs_country = pd.concat (pairs_country)\n",
    "                    \n",
    "            pairs_country = pairs_country.groupby ( [\"id\",\"match_id\"]).agg ( {\n",
    "                                                                f'xml_{vec_name}_kneighbors':[\"mean\"],\n",
    "                                                                f'xml_{vec_name}_kdist_mean_1': [\"mean\"],\n",
    "                                                                f'xml_{vec_name}_kdist_mean_2': [\"mean\"],\n",
    "                                                                f'xml_{vec_name}_kdist':[\"count\"]\n",
    "                                                                   } ).reset_index() \n",
    "\n",
    "            pairs_country.columns =  ['_'.join(col) for col in pairs_country.columns.values]\n",
    "            pairs_country = pairs_country.rename(columns={\"id_\":\"id\", \"match_id_\":\"match_id\", f'xml_{vec_name}_kdist': f\"xml_{vec_name}_count\"})\n",
    "            pairs.append(pairs_country)\n",
    "\n",
    "    pairs = pd.concat(pairs)\n",
    "    \n",
    "    print(f\"recall_knn_vec: {vec_name}: {pairs.shape}\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def recall_knn_tfid ( df, neighbors, total_neighbors, col ):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tv_fit = tfidf.fit_transform(df[col].fillna('nan'))\n",
    "    pairs = []\n",
    "    for country, country_df in tqdm(df.groupby('country')):\n",
    "        pairs_country = []\n",
    "        country_df = country_df[ country_df[col].notnull() ]\n",
    "        size = len(country_df)\n",
    "        total_neighbors_ = min ( size, total_neighbors)\n",
    "        neighbors_ = min(size, neighbors)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if total_neighbors_ > 1:        \n",
    "\n",
    "            knn = NearestNeighbors(n_neighbors = total_neighbors_, metric = 'cosine', algorithm=\"brute\", n_jobs = -1)\n",
    "            \n",
    "            #print (country, size, total_neighbors_, neighbors_)\n",
    "\n",
    "            \n",
    "            idx = country_df.index\n",
    "            knn.fit(tv_fit[idx])            \n",
    "            \n",
    "            dists, nears = knn.kneighbors(tv_fit[idx], return_distance = True)    \n",
    "            mean_1 = dists[:, :neighbors_].mean(axis=1)\n",
    "            mean_2 = dists[:, :total_neighbors_].mean(axis=1)\n",
    "            \n",
    "            for k in range(0,neighbors_):            \n",
    "                cur_df = country_df[['id']]\n",
    "                cur_df['match_id'] = country_df['id'].values[nears[:, k]]\n",
    "                cur_df[f'tfidf_{col}_kdist'] = dists[:, k]\n",
    "                cur_df[f'tfid_{col}_kneighbors'] = k\n",
    "                cur_df[f'tfid_{col}_kdist_mean_1'] = mean_1\n",
    "                cur_df[f'tfid_{col}_kdist_mean_2'] = mean_2\n",
    "                cur_df = cur_df.query(\"id != match_id\")\n",
    "                if len(cur_df) > 0:\n",
    "                    flag = cur_df[\"id\"] > cur_df[\"match_id\"]\n",
    "                    ids = cur_df[flag][\"id\"].values\n",
    "                    match_ids = cur_df[flag][\"match_id\"].values\n",
    "\n",
    "                    cur_df.loc[flag, \"id\"] = match_ids\n",
    "                    cur_df.loc[flag, \"match_id\"] = ids\n",
    "\n",
    "                    pairs_country.append(cur_df)\n",
    "            pairs_country = pd.concat (pairs_country)\n",
    "            pairs_country = pairs_country.groupby ( [\"id\",\"match_id\"]).agg ( {\n",
    "                                                            f'tfid_{col}_kneighbors':[\"mean\"],\n",
    "                                                            f'tfid_{col}_kdist_mean_1': [\"mean\"],\n",
    "                                                            f'tfid_{col}_kdist_mean_2': [\"mean\"],\n",
    "                                                            f'tfidf_{col}_kdist':[\"count\"]\n",
    "                                                               } ).reset_index() \n",
    "            pairs_country.columns =  ['_'.join(col) for col in pairs_country.columns.values]\n",
    "            pairs_country = pairs_country.rename(columns={\"id_\":\"id\", \"match_id_\":\"match_id\" })\n",
    "            pairs.append(pairs_country)\n",
    "            \n",
    "    pairs = pd.concat(pairs)\n",
    "    print(f\"recall_knn_tfid: {col}: {pairs.shape}\")\n",
    "    return pairs\n",
    "\n",
    "def recall_knn_tfid_char ( df, neighbors, total_neighbors, col ):\n",
    "    tfidf = TfidfVectorizer(ngram_range=(3, 3), analyzer=\"char_wb\", use_idf=False)\n",
    "    tv_fit = tfidf.fit_transform(df[col].fillna('nan'))\n",
    "    pairs = []\n",
    "    for country, country_df in tqdm(df.groupby('country')):\n",
    "        pairs_country = []\n",
    "        country_df = country_df[ country_df[col].notnull() ]\n",
    "        size = len(country_df)\n",
    "        total_neighbors_ = min ( size, total_neighbors)\n",
    "        neighbors_ = min(size, neighbors)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if total_neighbors_ > 1:        \n",
    "\n",
    "            knn = NearestNeighbors(n_neighbors = total_neighbors_, metric = 'cosine', algorithm=\"brute\", n_jobs = -1)\n",
    "            \n",
    "            #print (country, size, total_neighbors_, neighbors_)\n",
    "\n",
    "            \n",
    "            idx = country_df.index\n",
    "            knn.fit(tv_fit[idx])            \n",
    "            \n",
    "            dists, nears = knn.kneighbors(tv_fit[idx], return_distance = True)    \n",
    "            mean_1 = dists[:, :neighbors_].mean(axis=1)\n",
    "            mean_2 = dists[:, :total_neighbors_].mean(axis=1)\n",
    "            \n",
    "            for k in range(0,neighbors_):            \n",
    "                cur_df = country_df[['id']]\n",
    "                cur_df['match_id'] = country_df['id'].values[nears[:, k]]\n",
    "                cur_df[f'tfid_char_{col}_kdist'] = dists[:, k]\n",
    "                cur_df[f'tfid_char_{col}_kneighbors'] = k\n",
    "                cur_df[f'tfid_char_{col}_kdist_mean_1'] = mean_1\n",
    "                cur_df[f'tfid_char_{col}_kdist_mean_2'] = mean_2\n",
    "                cur_df = cur_df.query(\"id != match_id\")\n",
    "                if len(cur_df) > 0:\n",
    "                    flag = cur_df[\"id\"] > cur_df[\"match_id\"]\n",
    "                    ids = cur_df[flag][\"id\"].values\n",
    "                    match_ids = cur_df[flag][\"match_id\"].values\n",
    "\n",
    "                    cur_df.loc[flag, \"id\"] = match_ids\n",
    "                    cur_df.loc[flag, \"match_id\"] = ids\n",
    "\n",
    "                    pairs_country.append(cur_df)\n",
    "            pairs_country = pd.concat (pairs_country)\n",
    "            pairs_country = pairs_country.groupby ( [\"id\",\"match_id\"]).agg ( {\n",
    "                                                            f'tfid_char_{col}_kneighbors':[\"mean\"],\n",
    "                                                            f'tfid_char_{col}_kdist_mean_1': [\"mean\"],\n",
    "                                                            f'tfid_char_{col}_kdist_mean_2': [\"mean\"],\n",
    "                                                            f'tfid_char_{col}_kdist':[\"count\"]\n",
    "                                                               } ).reset_index() \n",
    "            pairs_country.columns =  ['_'.join(col) for col in pairs_country.columns.values]\n",
    "            pairs_country = pairs_country.rename(columns={\"id_\":\"id\", \"match_id_\":\"match_id\" })\n",
    "            pairs.append(pairs_country)\n",
    "            \n",
    "    pairs = pd.concat(pairs)\n",
    "    print(f\"recall_knn_tfid_char: {col}: {pairs.shape}\")\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def recall_knn(df, verbose=False):\n",
    "    \n",
    "    pairs = recall_knn_latlong (df, LATLONG_NEIGHBORS,  TOTAL_NEIGHBORS, LATLONG_THRESHOLD)\n",
    "\n",
    "    pairs = reduce_mem_usage(pairs)\n",
    "    print(f\"shape: {pairs.shape}\")\n",
    "\n",
    "    \n",
    "    #tfidf char name\n",
    "    data =  pd.read_feather( \"convert_data.feather\")\n",
    "    recall_df = recall_knn_tfid_char(data, NEIGHBORS, TOTAL_NEIGHBORS, \"name\")\n",
    "    recall_df = reduce_mem_usage(recall_df)\n",
    "    pairs = pairs.merge(recall_df, on=[\"id\",\"match_id\"], how=\"outer\")    \n",
    "    print(f\"tfidf_char name shape: {pairs.shape}\")\n",
    "    del data\n",
    "                       \n",
    "    #vec name\n",
    "    recall_df = recall_knn_vec(df, NEIGHBORS, TOTAL_NEIGHBORS_2, NAME_THRESHOLD, \"name\")\n",
    "    recall_df = reduce_mem_usage(recall_df)\n",
    "    pairs = pairs.merge(recall_df, on=[\"id\",\"match_id\"], how=\"outer\")\n",
    "    print(f\"vec name shape: {pairs.shape}\")\n",
    "\n",
    "    #vec address\n",
    "    recall_df = recall_knn_vec(df, NEIGHBORS, TOTAL_NEIGHBORS_2, ADDRESS_THRESHOLD, \"address\")\n",
    "    recall_df = reduce_mem_usage(recall_df)\n",
    "    pairs = pairs.merge(recall_df, on=[\"id\",\"match_id\"], how=\"outer\")\n",
    "    pairs = reduce_mem_usage(pairs)\n",
    "    print(f\"vec address shape: {pairs.shape}\")\n",
    "    \n",
    "    #vec categoris\n",
    "    \"\"\"\n",
    "    recall_df = recall_knn_vec(df,NEIGHBORS, TOTAL_NEIGHBORS_2, CATEGORIES_THRESHOLD, \"categories\")\n",
    "    recall_df = reduce_mem_usage(recall_df)\n",
    "    pairs = pairs.merge(recall_df, on=[\"id\",\"match_id\"], how=\"outer\")\n",
    "\n",
    "    pairs = reduce_mem_usage(pairs)\n",
    "    print(f\"vec categories shape: {pairs.shape}\")\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    print(pairs.dtypes)\n",
    "\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def generate_pairs (data):\n",
    "    \n",
    "    train_data = recall_knn(data, verbose=False)\n",
    "    \n",
    "\n",
    "    #generate target\n",
    "    if \"point_of_interest\" in data:\n",
    "        data = data.set_index('id')\n",
    "        ids = train_data['id'].tolist()\n",
    "        match_ids = train_data['match_id'].tolist()\n",
    "\n",
    "        poi = data.loc[ids]['point_of_interest'].values\n",
    "        match_poi = data.loc[match_ids]['point_of_interest'].values\n",
    "\n",
    "        train_data['label'] = np.array(poi == match_poi, dtype = np.int8)\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    data = pd.read_feather(\"data.feather\")\n",
    "\n",
    "\n",
    "    pairs = generate_pairs (data)\n",
    "\n",
    "\n",
    "    print(f\"pairs shape: {pairs.shape}\")\n",
    "\n",
    "    pairs = pairs.reset_index(drop=True)\n",
    "    print(\"save\")\n",
    "    pairs.to_feather(f\"pairs.feather\")\n",
    "    \n",
    "    \n",
    "SEED = 2022\n",
    "seed_everything(SEED)\n",
    "TOTAL_NEIGHBORS = 100\n",
    "TOTAL_NEIGHBORS_2 = 50\n",
    "LATLONG_NEIGHBORS = 12\n",
    "LATLONG_THRESHOLD = 0.000015\n",
    "NAME_THRESHOLD = 0.00099\n",
    "ADDRESS_THRESHOLD = 0.00492\n",
    "#CATEGORIES_THRESHOLD = 0.00001\n",
    "NEIGHBORS = 10\n",
    "TEXT2VEC_PATH = \".\"\n",
    "TEXT2VEC_PREFIX = \"sentence-transformers_paraphrase-xlm-r-multilingual-v1_\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b07f681",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:02:45.907652Z",
     "iopub.status.busy": "2022-07-05T07:02:45.907192Z",
     "iopub.status.idle": "2022-07-05T07:03:08.511931Z",
     "shell.execute_reply": "2022-07-05T07:03:08.510541Z"
    },
    "papermill": {
     "duration": 22.619191,
     "end_time": "2022-07-05T07:03:08.514204",
     "exception": false,
     "start_time": "2022-07-05T07:02:45.895013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuml:21.10.02\r\n",
      "100%|███████████████████████████████████████████| 32/32 [00:09<00:00,  3.55it/s]\r\n",
      "recall_knn_latlong: (291, 6)\r\n",
      "Memory usage after optimization is: 0.01 MB\r\n",
      "Decreased by 35.4%\r\n",
      "shape: (291, 6)\r\n",
      "100%|███████████████████████████████████████████| 32/32 [00:01<00:00, 31.98it/s]\r\n",
      "recall_knn_tfid_char: name: (281, 6)\r\n",
      "Memory usage after optimization is: 0.01 MB\r\n",
      "Decreased by 35.4%\r\n",
      "tfidf_char name shape: (365, 10)\r\n",
      "100%|███████████████████████████████████████████| 32/32 [00:03<00:00, 10.39it/s]\r\n",
      "recall_knn_vec: name: (262, 6)\r\n",
      "Memory usage after optimization is: 0.01 MB\r\n",
      "Decreased by 35.4%\r\n",
      "vec name shape: (397, 14)\r\n",
      "100%|███████████████████████████████████████████| 32/32 [00:02<00:00, 13.93it/s]\r\n",
      "recall_knn_vec: address: (189, 6)\r\n",
      "Memory usage after optimization is: 0.01 MB\r\n",
      "Decreased by 35.4%\r\n",
      "Memory usage after optimization is: 0.02 MB\r\n",
      "Decreased by 30.0%\r\n",
      "vec address shape: (412, 18)\r\n",
      "id                                   object\r\n",
      "match_id                             object\r\n",
      "kneighbors_mean                     float16\r\n",
      "kdist_mean_1_mean                   float16\r\n",
      "kdist_mean_2_mean                   float16\r\n",
      "kdist_count                         float16\r\n",
      "tfid_char_name_kneighbors_mean      float16\r\n",
      "tfid_char_name_kdist_mean_1_mean    float16\r\n",
      "tfid_char_name_kdist_mean_2_mean    float16\r\n",
      "tfid_char_name_kdist_count          float16\r\n",
      "xml_name_kneighbors_mean            float16\r\n",
      "xml_name_kdist_mean_1_mean          float16\r\n",
      "xml_name_kdist_mean_2_mean          float16\r\n",
      "xml_name_kdist_count                float16\r\n",
      "xml_address_kneighbors_mean         float16\r\n",
      "xml_address_kdist_mean_1_mean       float16\r\n",
      "xml_address_kdist_mean_2_mean       float16\r\n",
      "xml_address_kdist_count             float16\r\n",
      "dtype: object\r\n",
      "pairs shape: (412, 18)\r\n",
      "save\r\n",
      "CPU times: user 278 ms, sys: 76.5 ms, total: 355 ms\n",
      "Wall time: 22.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python generate_pairs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004451cf",
   "metadata": {
    "papermill": {
     "duration": 0.014788,
     "end_time": "2022-07-05T07:03:08.544883",
     "exception": false,
     "start_time": "2022-07-05T07:03:08.530095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c928f79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:03:08.575406Z",
     "iopub.status.busy": "2022-07-05T07:03:08.575082Z",
     "iopub.status.idle": "2022-07-05T07:03:08.586355Z",
     "shell.execute_reply": "2022-07-05T07:03:08.585621Z"
    },
    "papermill": {
     "duration": 0.028763,
     "end_time": "2022-07-05T07:03:08.588032",
     "exception": false,
     "start_time": "2022-07-05T07:03:08.559269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing generate_fe.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generate_fe.py\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)    \n",
    "    \n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    if verbose:\n",
    "        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cosine_similarity (a,b):\n",
    "    \"\"\"\n",
    "    a /= np.linalg.norm(a, axis=1).reshape((-1, 1))\n",
    "    b /= np.linalg.norm(b, axis=1).reshape((-1, 1))\n",
    "\n",
    "    cos_sim = np.sum(a*b, axis=1)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    t1 = torch.from_numpy(a).to(DEVICE)\n",
    "    t2 = torch.from_numpy(b).to(DEVICE)\n",
    "    cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    output = cos(t1, t2)\n",
    "    return output.to('cpu').numpy()\n",
    "\n",
    "def lat_lon_cos_sim (pairs, inv_d, path):\n",
    "\n",
    "    file = open(path,'rb')\n",
    "    lat_lon_vec = pickle.load(file)\n",
    "\n",
    "    ids1_x = pairs[\"id\"].map(inv_d)\n",
    "    ids2_x = pairs[\"match_id\"].map(inv_d)\n",
    "\n",
    "\n",
    "    a = lat_lon_vec[ids1_x]\n",
    "    b = lat_lon_vec[ids2_x]\n",
    "    #cos_sim = np.sum(a*b, axis=1)\n",
    "    #return cos_sim\n",
    "    return cosine_similarity(a,b)\n",
    "\n",
    "def get_vec (feat_series, feat_name, prefix  ):\n",
    "    \n",
    "    df_feat = pd.read_csv(f'{prefix}{feat_name}.csv')\n",
    "    file = open(f'{prefix}{feat_name}.vec','rb')\n",
    "    feat_vec = pickle.load(file)\n",
    "\n",
    "    \n",
    "    d = df_feat[feat_name].to_dict()\n",
    "    inv_d = {v: k for k, v in d.items()}\n",
    "\n",
    "    feat_values = feat_series.values\n",
    "\n",
    "    vec = np.zeros((feat_values.shape[0], feat_vec.shape[1]))\n",
    "    for i in range(vec.shape[0]):\n",
    "        vec[i,:] = feat_vec[ inv_d[feat_values[i]], : ]\n",
    "    return vec\n",
    "\n",
    "def feat_cos_sim(feat_name,  prefix , feat_series , inv_d, pairs, batch_size):\n",
    "    vec = get_vec ( feat_series,feat_name = feat_name, prefix = prefix )    \n",
    "    step = len(pairs)//batch_size\n",
    "    if batch_size*step < len(pairs):\n",
    "        step += 1\n",
    "\n",
    "    ret = []\n",
    "    start = 0\n",
    "    for i in tqdm(range(step)):\n",
    "        end = start + batch_size\n",
    "\n",
    "        ids1_x = pairs[start:end][\"id\"].map(inv_d).values\n",
    "        ids2_x = pairs[start:end][\"match_id\"].map(inv_d).values\n",
    "\n",
    "        a = vec[ids1_x] \n",
    "        b = vec[ids2_x] \n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        a /= np.linalg.norm(a, axis=1).reshape((-1, 1))\n",
    "        b /= np.linalg.norm(b, axis=1).reshape((-1, 1))\n",
    "\n",
    "        cos_sim = np.sum(a*b, axis=1)\n",
    "        \"\"\"\n",
    "\n",
    "        cos_sim = cosine_similarity(a,b)\n",
    "\n",
    "        \n",
    "        ret.append(cos_sim)\n",
    "        start += batch_size\n",
    "    \n",
    "    return np.concatenate(ret)\n",
    "\n",
    "\n",
    "def generate_fe (path, batch_size = 100_000):\n",
    "    data = pd.read_feather(\"data.feather\")\n",
    "    d = data[\"id\"].to_dict()\n",
    "    # train[\"id\"] --> train.index\n",
    "    inv_d = {v: k for k, v in d.items()}\n",
    "    data.index = data[\"id\"]\n",
    "\n",
    "    pairs = pd.read_feather(\"pairs.feather\")\n",
    "    if DEBUG:\n",
    "        pairs = pairs [:1000]\n",
    "    \n",
    "    if \"label\" in pairs:\n",
    "        targets = pairs[\"label\"].values\n",
    "        pairs = pairs.drop([\"label\"], axis = 1)\n",
    "    else:\n",
    "        targets = None\n",
    "    \n",
    "    \n",
    "    pairs = pairs.rename(columns={\"id_count\":\"kdist_count\"})\n",
    "\n",
    "    for feat_name in [\"address\", \"city\", \"categories\"]:\n",
    "        print(feat_name)\n",
    "        d = data[feat_name].notnull().to_dict()\n",
    "        pairs[f\"{feat_name}_id_notnull\"] = pairs[\"id\"].map(d).astype(np.int8)\n",
    "        pairs[f\"{feat_name}_match_id_notnull\"] = pairs[\"match_id\"].map(d).astype(np.int8)\n",
    "    pairs = reduce_mem_usage (pairs)\n",
    "\n",
    "    print(\"lat_lon_cs\")\n",
    "    cos_sim = lat_lon_cos_sim (pairs, inv_d, path=f\"{INPUT_VEC}/lat_lon.vec\")\n",
    "    pairs[\"lat_lon_cs\"] = cos_sim\n",
    "    pairs = reduce_mem_usage (pairs)\n",
    "\n",
    "    postfix = \"xml\"\n",
    "    prefix = \"./sentence-transformers_paraphrase-xlm-r-multilingual-v1_\"\n",
    "\n",
    "    for feat_name in [ \"name\", \"address\", \"categories\", \"city\" ]:\n",
    "        print(feat_name)\n",
    "        cos_sim = feat_cos_sim (feat_name = feat_name, prefix = prefix , \n",
    "                                feat_series = data[feat_name].fillna(\"unknow\"), \n",
    "                                inv_d = inv_d, \n",
    "                                pairs = pairs, \n",
    "                                batch_size = batch_size)\n",
    "        pairs[f\"{feat_name}_{postfix}_cs\"] = cos_sim\n",
    "        pairs = reduce_mem_usage ( pairs , verbose=True)\n",
    "\n",
    "    postfix = \"mpnet-ml\"\n",
    "    path = \"./sentence-transformers_paraphrase-multilingual-mpnet-base-v2_\"\n",
    "\n",
    "    for feat_name in [ \"name\", \"address\", \"categories\", \"city\" ]:\n",
    "        print(feat_name)\n",
    "        cos_sim = feat_cos_sim (feat_name = feat_name, prefix = prefix , \n",
    "                                feat_series = data[feat_name].fillna(\"unknow\"), \n",
    "                                inv_d = inv_d, \n",
    "                                pairs = pairs, \n",
    "                                batch_size = batch_size)\n",
    "        pairs[f\"{feat_name}_{postfix}_cs\"] = cos_sim\n",
    "        pairs = reduce_mem_usage ( pairs , verbose=True)\n",
    "\n",
    "    postfix = \"mpnet\"\n",
    "    path = \"./sentence-transformers_all-mpnet-base-v2_\"\n",
    "\n",
    "    for feat_name in [ \"name\", \"address\", \"categories\", \"city\" ]:\n",
    "        print(feat_name)\n",
    "        cos_sim = feat_cos_sim (feat_name = feat_name, prefix = prefix, \n",
    "                                feat_series = data[feat_name].fillna(\"unknow\"), \n",
    "                                inv_d = inv_d, \n",
    "                                pairs = pairs, \n",
    "                                batch_size = batch_size)\n",
    "        pairs[f\"{feat_name}_{postfix}_cs\"] = cos_sim\n",
    "        pairs = reduce_mem_usage ( pairs , verbose=True)\n",
    "\n",
    "    if not targets is None:\n",
    "        pairs[\"label\"] = targets\n",
    "        \n",
    "    return pairs \n",
    "\n",
    "\n",
    "\n",
    "SEED = 2022\n",
    "seed_everything(SEED)\n",
    "\n",
    "INPUT_PATH = \"./\"\n",
    "INPUT_VEC = \"./\"\n",
    "DEBUG = False\n",
    "\n",
    "def main():\n",
    "    pairs = generate_fe(path = INPUT_PATH,  batch_size = 200_000)\n",
    "    pairs.to_feather(f\"pairs.feather\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adcbc75f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:03:08.618474Z",
     "iopub.status.busy": "2022-07-05T07:03:08.617914Z",
     "iopub.status.idle": "2022-07-05T07:03:12.047560Z",
     "shell.execute_reply": "2022-07-05T07:03:12.046493Z"
    },
    "papermill": {
     "duration": 3.446884,
     "end_time": "2022-07-05T07:03:12.049432",
     "exception": false,
     "start_time": "2022-07-05T07:03:08.602548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "address\r\n",
      "city\r\n",
      "categories\r\n",
      "Memory usage after optimization is: 0.02 MB\r\n",
      "Decreased by 0.0%\r\n",
      "lat_lon_cs\r\n",
      "Memory usage after optimization is: 0.02 MB\r\n",
      "Decreased by 9.6%\r\n",
      "name\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 136.36it/s]\r\n",
      "Memory usage after optimization is: 0.02 MB\r\n",
      "Decreased by 9.3%\r\n",
      "address\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 306.69it/s]\r\n",
      "Memory usage after optimization is: 0.02 MB\r\n",
      "Decreased by 9.0%\r\n",
      "categories\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 321.08it/s]\r\n",
      "Memory usage after optimization is: 0.02 MB\r\n",
      "Decreased by 8.8%\r\n",
      "city\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 313.92it/s]\r\n",
      "Memory usage after optimization is: 0.03 MB\r\n",
      "Decreased by 8.5%\r\n",
      "name\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 315.22it/s]\r\n",
      "Memory usage after optimization is: 0.03 MB\r\n",
      "Decreased by 8.3%\r\n",
      "address\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 308.31it/s]\r\n",
      "Memory usage after optimization is: 0.03 MB\r\n",
      "Decreased by 8.1%\r\n",
      "categories\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 301.36it/s]\r\n",
      "Memory usage after optimization is: 0.03 MB\r\n",
      "Decreased by 7.9%\r\n",
      "city\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 305.84it/s]\r\n",
      "Memory usage after optimization is: 0.03 MB\r\n",
      "Decreased by 7.7%\r\n",
      "name\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 307.39it/s]\r\n",
      "Memory usage after optimization is: 0.03 MB\r\n",
      "Decreased by 7.5%\r\n",
      "address\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 240.83it/s]\r\n",
      "Memory usage after optimization is: 0.03 MB\r\n",
      "Decreased by 7.3%\r\n",
      "categories\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 307.77it/s]\r\n",
      "Memory usage after optimization is: 0.03 MB\r\n",
      "Decreased by 7.1%\r\n",
      "city\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 311.54it/s]\r\n",
      "Memory usage after optimization is: 0.03 MB\r\n",
      "Decreased by 7.0%\r\n",
      "CPU times: user 44.8 ms, sys: 9.86 ms, total: 54.6 ms\n",
      "Wall time: 3.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python generate_fe.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8c79a18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:03:12.082031Z",
     "iopub.status.busy": "2022-07-05T07:03:12.081661Z",
     "iopub.status.idle": "2022-07-05T07:03:12.085887Z",
     "shell.execute_reply": "2022-07-05T07:03:12.085030Z"
    },
    "papermill": {
     "duration": 0.022867,
     "end_time": "2022-07-05T07:03:12.087730",
     "exception": false,
     "start_time": "2022-07-05T07:03:12.064863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b6800",
   "metadata": {
    "papermill": {
     "duration": 0.014704,
     "end_time": "2022-07-05T07:03:12.118416",
     "exception": false,
     "start_time": "2022-07-05T07:03:12.103712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Extraction EXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7003a539",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:03:12.149949Z",
     "iopub.status.busy": "2022-07-05T07:03:12.149685Z",
     "iopub.status.idle": "2022-07-05T07:03:12.163107Z",
     "shell.execute_reply": "2022-07-05T07:03:12.162331Z"
    },
    "papermill": {
     "duration": 0.03116,
     "end_time": "2022-07-05T07:03:12.164757",
     "exception": false,
     "start_time": "2022-07-05T07:03:12.133597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing generate_fe_ext.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generate_fe_ext.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import Levenshtein\n",
    "import difflib\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import joblib\n",
    "\n",
    "def calculate_jaccard_char(str1, str2):\n",
    "    \n",
    "    # Combine both tokens to find union.\n",
    "    both_tokens = str1 + str2\n",
    "    union = set(both_tokens)\n",
    "    if len(union) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate intersection.\n",
    "    intersection = set()\n",
    "    for w in set(str1):\n",
    "        if w in set(str2):\n",
    "            intersection.add(w)\n",
    "\n",
    "    jaccard_score = len(intersection)/len(union)\n",
    "    \n",
    "    return jaccard_score\n",
    "\n",
    "def calculate_jaccard_word(str1, str2):\n",
    "    \n",
    "    # Combine both tokens to find union.\n",
    "    words1 = str1.split()\n",
    "    words2 = str2.split()\n",
    "    union = set(words1 + words2)\n",
    "    if len(union) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate intersection.\n",
    "    intersection = set()\n",
    "    for word in union:\n",
    "        if word in words1 and word in words2:\n",
    "            intersection.add(word)\n",
    "\n",
    "    jaccard_score = len(intersection)/len(union)\n",
    "    \n",
    "    return jaccard_score\n",
    "    \n",
    "def calculate_jaccard_word_smallest(str1, str2):\n",
    "    \n",
    "    if str1 == str2:\n",
    "        return 1\n",
    "    \n",
    "    # Combine both tokens to find union.\n",
    "    words1 = str1.split()\n",
    "    words2 = str2.split()\n",
    "    union = set(words1 + words2)\n",
    "    small = min(len(set(words1)), len(set(words2)))\n",
    "    if small == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate intersection.\n",
    "    intersection = set()\n",
    "    for word in union:\n",
    "        if word in words1 and word in words2:\n",
    "            intersection.add(word)\n",
    "\n",
    "    jaccard_score = len(intersection)/small\n",
    "    \n",
    "    return jaccard_score\n",
    "\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def generate_fe_ext (train_pairs, batch_size ):\n",
    "    def process_tfidf (col):\n",
    "        \n",
    "        train = pd.read_feather(f\"col_{col}.feather\")\n",
    "        \n",
    "        tfidf = TfidfVectorizer()\n",
    "        tv_fit = tfidf.fit_transform(train[col].fillna('nan'))\n",
    "        a = np.array(tv_fit[indexs].multiply(tv_fit[match_indexs]).sum(axis = 1)).ravel().astype(np.float16)\n",
    "        np.save(f\"{col}_tfid.npy\", a)\n",
    "        \n",
    "        del train, tfidf, a\n",
    "        gc.collect()    \n",
    "\n",
    "    def process_tfidf_char (col):\n",
    "        \n",
    "        train = pd.read_feather(f\"conv_col_{col}.feather\")\n",
    "        \n",
    "        tfidf = TfidfVectorizer(ngram_range=(3, 3), analyzer=\"char_wb\", use_idf=False)\n",
    "        tv_fit = tfidf.fit_transform(train[col].fillna('nan'))        \n",
    "\n",
    "        a = np.array(tv_fit[indexs].multiply(tv_fit[match_indexs]).sum(axis = 1)).ravel().astype(np.float16)\n",
    "        np.save(f\"conv_{col}_tfid.npy\", a)\n",
    "        \n",
    "        del train, tfidf, a\n",
    "        gc.collect()    \n",
    "        \n",
    "        \n",
    "\n",
    "    def process_ext(col):\n",
    "        \n",
    "        with open(\"log.txt\", 'a') as f:\n",
    "            f.write(f\"process_ext: start {col}\\n\")\n",
    "        \n",
    "        train = pd.read_feather(f\"col_{col}.feather\")\n",
    "\n",
    "        ret_gesh = []\n",
    "        ret_levens = []\n",
    "        ret_jaros = []\n",
    "        ret_lcs = []\n",
    "        ret_jaccard_words = []\n",
    "        ret_jaccard_chars = []        \n",
    "\n",
    "\n",
    "        start = 0\n",
    "        for i in range(step):\n",
    "            gesh = []\n",
    "            levens = []\n",
    "            jaros = []\n",
    "            lcs = []\n",
    "            jaccard_words = []\n",
    "            jaccard_chars = []\n",
    "\n",
    "            end = start + batch_size          \n",
    "            id_values = train.loc[indexs[start:end]][col].values.astype(str)\n",
    "            match_id_values = train.loc[match_indexs[start:end]][col].values.astype(str)\n",
    "\n",
    "\n",
    "            \n",
    "            for s, match_s in zip(id_values, match_id_values):\n",
    "                if s != 'nan' and match_s != 'nan':                    \n",
    "                    sm = difflib.SequenceMatcher(None, s, match_s)\n",
    "                    \n",
    "                    gesh.append(sm.ratio())\n",
    "                    levens.append(Levenshtein.distance(s, match_s))\n",
    "                    jaros.append(Levenshtein.jaro_winkler(s, match_s))                    \n",
    "                    lcs.append(sm.find_longest_match(0,len(s), 0,len(match_s)).size)\n",
    "                    \n",
    "                    if col in [\"name\", \"address\", \"categories\"]:\n",
    "                        jaccard_words.append(calculate_jaccard_word(s, match_s))\n",
    "                        jaccard_chars.append(calculate_jaccard_char(s, match_s))                    \n",
    "                else:\n",
    "                    gesh.append(np.nan)\n",
    "                    levens.append(np.nan)\n",
    "                    jaros.append(np.nan)\n",
    "                    lcs.append(np.nan)\n",
    "                    if col in [\"name\", \"address\", \"categories\"]:\n",
    "                        jaccard_words.append(np.nan)\n",
    "                        jaccard_chars.append(np.nan)        \n",
    "                    \n",
    "            start += batch_size            \n",
    "            \n",
    "            ret_gesh.append(np.array(gesh).astype(np.float16))\n",
    "            ret_levens.append(np.array(levens).astype(np.float16))\n",
    "            ret_jaros.append(np.array(jaros).astype(np.float16))\n",
    "            ret_lcs.append(np.array(lcs).astype(np.float16))\n",
    "            del gesh, levens, jaros, lcs\n",
    "\n",
    "            if col in [\"name\", \"address\", \"categories\"]:\n",
    "                ret_jaccard_words.append(np.array(jaccard_words).astype(np.float16))\n",
    "                ret_jaccard_chars.append(np.array(jaccard_chars).astype(np.float16))\n",
    "            \n",
    "            del jaccard_words, jaccard_chars\n",
    "            gc.collect()\n",
    "            \n",
    "        np.save(f\"{col}_gesh.npy\", np.concatenate(ret_gesh))\n",
    "        np.save(f\"{col}_levens.npy\", np.concatenate(ret_levens))\n",
    "        np.save(f\"{col}_jeros.npy\", np.concatenate(ret_jaros))\n",
    "        np.save(f\"{col}_lcs.npy\", np.concatenate(ret_lcs))\n",
    "        del train, ret_gesh, ret_levens, ret_jaros, ret_lcs\n",
    "\n",
    "        if col in [\"name\", \"address\", \"categories\"]:        \n",
    "            np.save(f\"{col}_jaccard_words.npy\", np.concatenate(ret_jaccard_words))\n",
    "            np.save(f\"{col}_jaccard_chars.npy\", np.concatenate(ret_jaccard_chars))        \n",
    "            del ret_jaccard_words, ret_jaccard_chars\n",
    "        \n",
    "        gc.collect()            \n",
    "        with open(\"log.txt\", \"a\") as f:\n",
    "            f.write(f\"process_ext: end {col}\\n\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    train = pd.read_feather(\"data.feather\")\n",
    "    \n",
    "    #train_pairs = train_pairs.drop([\"xml_name_kdist\", \"xml_address_kdist\", \"xml_categories_kdist\"], axis=1)\n",
    "    #train_pairs = reduce_mem_usage (train_pairs, verbose = True)\n",
    "    tfidf_cols = [\"name\", \"address\", \"categories\", 'state', 'zip', 'url', 'phone' ] \n",
    "    tfidf_conv_cols = [\"name\", \"address\", \"city\", \"state\"] \n",
    "    feat_cols = [\"name\", \"address\", \"categories\", 'state', 'zip', 'url', 'phone']\n",
    "    \n",
    "    \n",
    "    d = train[\"id\"].to_dict()\n",
    "    # train[\"id\"] --> train.index\n",
    "    inv_d = {v: k for k, v in d.items()}\n",
    "\n",
    "    indexs = [inv_d[i] for i in train_pairs['id']]\n",
    "    match_indexs = [inv_d[i] for i in train_pairs['match_id']]\n",
    "    del d, inv_d \n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"save cols\")\n",
    "    for col in set(tfidf_cols+feat_cols):\n",
    "        print(col)\n",
    "        train[[col]].to_feather(f\"col_{col}.feather\")\n",
    "    \n",
    "    del train\n",
    "    gc.collect()    \n",
    "    \n",
    "    train = pd.read_feather(\"convert_data.feather\")\n",
    "    print(\"save conv cols\")\n",
    "    for col in set(tfidf_conv_cols):\n",
    "        print(col)\n",
    "        train[[col]].to_feather(f\"conv_col_{col}.feather\")\n",
    "    \n",
    "    del train\n",
    "    gc.collect()        \n",
    "\n",
    "    \n",
    "    print(\"TF-IDF char feats\")\n",
    "    joblib.Parallel(n_jobs=2, timeout=9999999)(joblib.delayed(process_tfidf_char)(col) for col in tfidf_conv_cols)  \n",
    "    \n",
    "    \n",
    "    print(\"TF-IDF feats\")\n",
    "    joblib.Parallel(n_jobs=2, timeout=9999999)(joblib.delayed(process_tfidf)(col) for col in tfidf_cols)  \n",
    "\n",
    "\n",
    "    print(\"Distance feats\")\n",
    "    #train.index = train[\"id\"]\n",
    "\n",
    "    step = train_pairs.shape[0]//batch_size\n",
    "    if batch_size*step < train_pairs.shape[0]:\n",
    "        step += 1\n",
    "    \n",
    "    joblib.Parallel(n_jobs=2, timeout=9999999)(joblib.delayed(process_ext)(col) for col in feat_cols)\n",
    "\n",
    "    del indexs, match_indexs\n",
    "    gc.collect()    \n",
    "\n",
    "    print(\"loading TF-IDF conv feats\")\n",
    "    for col in tfidf_conv_cols:\n",
    "        print(f\"loading {col}\" )\n",
    "        train_pairs[f'conv_{col}_tfid'] =  np.load(f'conv_{col}_tfid.npy')\n",
    "    train_pairs = reduce_mem_usage (train_pairs, verbose = False)    \n",
    "    \n",
    "    print(\"loading TF-IDF feats\")\n",
    "    for col in tfidf_cols:\n",
    "        print(f\"loading {col}\" )\n",
    "        train_pairs[f'{col}_tfid'] =  np.load(f'{col}_tfid.npy')\n",
    "    train_pairs = reduce_mem_usage (train_pairs, verbose = False)\n",
    "    \n",
    "    print(\"loading Distance feats\")\n",
    "    for col in feat_cols:\n",
    "\n",
    "        print(f\"loading {col}\" )\n",
    "        \n",
    "        train_pairs[f\"{col}_gesh\"] = np.load(f\"{col}_gesh.npy\")\n",
    "        train_pairs[f\"{col}_levens\"] =  np.load(f\"{col}_levens.npy\")\n",
    "        train_pairs[f\"{col}_jeros\"] =  np.load(f\"{col}_jeros.npy\")\n",
    "        train_pairs[f\"{col}_lcs\"] =  np.load(f\"{col}_lcs.npy\")   \n",
    "    \n",
    "        if col in [\"name\", \"address\", \"categories\"]:       \n",
    "            train_pairs[f\"{col}_jaccard_words\"] =  np.load(f\"{col}_jaccard_words.npy\")\n",
    "            train_pairs[f\"{col}_jaccard_chars\"] =  np.load(f\"{col}_jaccard_chars.npy\")               \n",
    "        train_pairs = reduce_mem_usage (train_pairs, verbose=False)   \n",
    "            \n",
    "    train = pd.read_feather(\"data.feather\")            \n",
    "    train.index = train[\"id\"]         \n",
    "    print(\"create len feats\")        \n",
    "    for col in feat_cols:\n",
    "        print(col)        \n",
    "        train[f\"len_{col}\"]=train[col].fillna(\"\").map(lambda x:len(x))\n",
    "        d=train[f\"len_{col}\"].to_dict()\n",
    "        train_pairs[f\"{col}_len_diff\"] = np.abs( train_pairs[\"id\"].map(d) - train_pairs[\"match_id\"].map(d)  )    \n",
    "\n",
    "\n",
    "    country_size = train.groupby(\"country\").size()\n",
    "    train.index=train[\"id\"]\n",
    "    d=train[\"country\"].to_dict()\n",
    "\n",
    "    train_pairs[\"country_id_size\"] = train_pairs[\"id\"].map(lambda x: country_size[d[x]])/150_000\n",
    "    del d\n",
    "    train_pairs = reduce_mem_usage (train_pairs, verbose = True)\n",
    "\n",
    "    print(\"lat/long feats\")\n",
    "    d_train_lat = train[\"latitude\"].to_dict()\n",
    "    d_train_long = train[\"longitude\"].to_dict()\n",
    "\n",
    "    train_pairs[\"lat_id\"] = train_pairs[\"id\"].map(d_train_lat)\n",
    "    train_pairs[\"lat_match_id\"] = train_pairs[\"match_id\"].map(d_train_lat)\n",
    "    train_pairs[\"long_id\"] = train_pairs[\"id\"].map(d_train_long)\n",
    "    train_pairs[\"long_match_id\"] = train_pairs[\"match_id\"].map(d_train_long)\n",
    "    \n",
    "    del d_train_lat, d_train_long\n",
    "    train_pairs[\"euclidean\"] = np.sqrt(((train_pairs['lat_id'] - train_pairs['lat_match_id'])**2) + ((train_pairs['long_id'] - train_pairs['long_match_id'])**2))\n",
    "    train_pairs[\"diff_lat\"] = np.abs(train_pairs['lat_id'] - train_pairs['lat_match_id'])\n",
    "    train_pairs[\"diff_long\"] = np.abs(train_pairs['long_match_id'] - train_pairs['long_id'])    \n",
    "    \n",
    "    train_pairs = reduce_mem_usage (train_pairs, verbose = True)\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"size feats\")    \n",
    "   \n",
    "    for feat in [\"name\", \"address\", \"url\"]:\n",
    "        print(feat)\n",
    "        df = train.groupby([feat,\"country\"]).agg({\"id\":\"count\"}).reset_index()\n",
    "        df = df.query(\"id >1\").reset_index()\n",
    "        df = df.rename(columns={\"id\":f\"{feat}_country_size\"})\n",
    "        df = train.merge( df, on = feat, how=\"left\" )\n",
    "        df[f\"{feat}_country_size\"] =   df[f\"{feat}_country_size\"].fillna(1)/10_000\n",
    "        df.index = df[\"id\"]    \n",
    "        d = df[f\"{feat}_country_size\"].to_dict()\n",
    "        del df\n",
    "        train_pairs[f\"{feat}_country_size_id\"] = train_pairs[\"id\"].map(d)\n",
    "        train_pairs[f\"{feat}_country_size_match_id\"] = train_pairs[\"match_id\"].map(d)\n",
    "        del d    \n",
    "        train_pairs = reduce_mem_usage (train_pairs, verbose = True)\n",
    "        gc.collect()    \n",
    "\n",
    "        df = train.groupby([feat]).agg({\"id\":\"count\"}).reset_index()\n",
    "        df = df.query(\"id >1\").reset_index()\n",
    "        df = df.rename(columns={\"id\":f\"{feat}_size\"})\n",
    "        df = train.merge( df, on = feat, how=\"left\" )\n",
    "        df[f\"{feat}_size\"] =   df[f\"{feat}_size\"].fillna(1)/10_000\n",
    "        df.index = df[\"id\"]    \n",
    "        d = df[f\"{feat}_size\"].to_dict()\n",
    "        del df\n",
    "        train_pairs[f\"{feat}_size_id\"] = train_pairs[\"id\"].map(d)\n",
    "        train_pairs[f\"{feat}_size_match_id\"] = train_pairs[\"match_id\"].map(d)\n",
    "        del d    \n",
    "        train_pairs = reduce_mem_usage (train_pairs, verbose = True)\n",
    "        gc.collect()    \n",
    "    \n",
    "    \n",
    "    return train_pairs\n",
    "\n",
    "def main():\n",
    "    DEBUG = False\n",
    "    pairs  = pd.read_feather(\"pairs.feather\")\n",
    "    if DEBUG :\n",
    "        pairs = pairs [:66333]\n",
    "\n",
    "\n",
    "    pairs = generate_fe_ext (pairs, batch_size = 50_000)\n",
    "    pairs.to_feather (\"pairs.feather\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d20502f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:03:12.196909Z",
     "iopub.status.busy": "2022-07-05T07:03:12.196251Z",
     "iopub.status.idle": "2022-07-05T07:03:16.803011Z",
     "shell.execute_reply": "2022-07-05T07:03:16.801688Z"
    },
    "papermill": {
     "duration": 4.624837,
     "end_time": "2022-07-05T07:03:16.804920",
     "exception": false,
     "start_time": "2022-07-05T07:03:12.180083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save cols\r\n",
      "url\r\n",
      "address\r\n",
      "zip\r\n",
      "categories\r\n",
      "phone\r\n",
      "state\r\n",
      "name\r\n",
      "save conv cols\r\n",
      "address\r\n",
      "state\r\n",
      "city\r\n",
      "name\r\n",
      "TF-IDF char feats\r\n",
      "TF-IDF feats\r\n",
      "Distance feats\r\n",
      "loading TF-IDF conv feats\r\n",
      "loading name\r\n",
      "loading address\r\n",
      "loading city\r\n",
      "loading state\r\n",
      "loading TF-IDF feats\r\n",
      "loading name\r\n",
      "loading address\r\n",
      "loading categories\r\n",
      "loading state\r\n",
      "loading zip\r\n",
      "loading url\r\n",
      "loading phone\r\n",
      "loading Distance feats\r\n",
      "loading name\r\n",
      "loading address\r\n",
      "loading categories\r\n",
      "loading state\r\n",
      "loading zip\r\n",
      "loading url\r\n",
      "loading phone\r\n",
      "create len feats\r\n",
      "name\r\n",
      "address\r\n",
      "categories\r\n",
      "state\r\n",
      "zip\r\n",
      "url\r\n",
      "phone\r\n",
      "Memory usage after optimization is: 0.07 MB\r\n",
      "Decreased by 23.0%\r\n",
      "lat/long feats\r\n",
      "Memory usage after optimization is: 0.08 MB\r\n",
      "Decreased by 17.8%\r\n",
      "size feats\r\n",
      "name\r\n",
      "Memory usage after optimization is: 0.08 MB\r\n",
      "Decreased by 5.7%\r\n",
      "Memory usage after optimization is: 0.08 MB\r\n",
      "Decreased by 5.6%\r\n",
      "address\r\n",
      "Memory usage after optimization is: 0.08 MB\r\n",
      "Decreased by 5.5%\r\n",
      "Memory usage after optimization is: 0.08 MB\r\n",
      "Decreased by 5.4%\r\n",
      "url\r\n",
      "Memory usage after optimization is: 0.08 MB\r\n",
      "Decreased by 5.3%\r\n",
      "Memory usage after optimization is: 0.09 MB\r\n",
      "Decreased by 5.2%\r\n",
      "CPU times: user 59 ms, sys: 11.2 ms, total: 70.2 ms\n",
      "Wall time: 4.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python generate_fe_ext.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc394b",
   "metadata": {
    "papermill": {
     "duration": 0.024287,
     "end_time": "2022-07-05T07:03:16.849123",
     "exception": false,
     "start_time": "2022-07-05T07:03:16.824836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a095936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:03:16.884691Z",
     "iopub.status.busy": "2022-07-05T07:03:16.884360Z",
     "iopub.status.idle": "2022-07-05T07:03:16.896824Z",
     "shell.execute_reply": "2022-07-05T07:03:16.896079Z"
    },
    "papermill": {
     "duration": 0.031951,
     "end_time": "2022-07-05T07:03:16.898544",
     "exception": false,
     "start_time": "2022-07-05T07:03:16.866593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predict.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgbm\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage before optimization is: {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    if verbose:\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def predict_xgb (model_name, pairs, cols, batch_size = 64):\n",
    "    import torch\n",
    "    from tqdm import tqdm\n",
    "    if torch.cuda.is_available():\n",
    "        from cuml import ForestInference\n",
    "        model = ForestInference.load(filename=model_name,model_type='xgboost')\n",
    "        step = pairs.shape[0]//batch_size\n",
    "        if batch_size*step < pairs.shape[0]:\n",
    "            step += 1\n",
    "        ret = []\n",
    "        start = 0\n",
    "        for i in tqdm(range(step)):\n",
    "            end = start + batch_size        \n",
    "            \n",
    "            X = pairs[start:end][cols].values\n",
    "            \n",
    "            pred = model.predict(X)\n",
    "            \n",
    "            ret.append(np.asarray(pred).astype(np.float16))\n",
    "            start += batch_size\n",
    "        pred = np.concatenate(ret)       \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        model = lgbm.Booster(model_file=model_name)\n",
    "        step = pairs.shape[0]//batch_size\n",
    "        if batch_size*step < pairs.shape[0]:\n",
    "            step += 1\n",
    "        ret = []\n",
    "        start = 0\n",
    "        for i in tqdm(range(step)):\n",
    "            end = start + batch_size        \n",
    "            \n",
    "            X = pairs[start:end][cols].values\n",
    "            \n",
    "            pred = model.predict(X)\n",
    "            \n",
    "            ret.append(np.asarray(pred).astype(np.float16))\n",
    "            start += batch_size\n",
    "        pred = np.concatenate(ret)               \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    #print(pred.shape, type(pred))    \n",
    "        \n",
    "    return pred\n",
    "\n",
    "\n",
    "def predict_lgb (model_name, pairs, cols, batch_size = 64):\n",
    "    import torch\n",
    "    from tqdm import tqdm\n",
    "    if torch.cuda.is_available():\n",
    "        from cuml import ForestInference\n",
    "        model = ForestInference.load(filename=model_name,model_type='lightgbm')\n",
    "        step = pairs.shape[0]//batch_size\n",
    "        if batch_size*step < pairs.shape[0]:\n",
    "            step += 1\n",
    "        ret = []\n",
    "        start = 0\n",
    "        for i in tqdm(range(step)):\n",
    "            end = start + batch_size        \n",
    "            \n",
    "            X = pairs[start:end][cols].values\n",
    "            \n",
    "            pred = model.predict(X)\n",
    "            \n",
    "            ret.append(np.asarray(pred).astype(np.float16))\n",
    "            start += batch_size\n",
    "        pred = np.concatenate(ret)       \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        model = lgbm.Booster(model_file=model_name)\n",
    "        step = pairs.shape[0]//batch_size\n",
    "        if batch_size*step < pairs.shape[0]:\n",
    "            step += 1\n",
    "        ret = []\n",
    "        start = 0\n",
    "        for i in tqdm(range(step)):\n",
    "            end = start + batch_size        \n",
    "            \n",
    "            X = pairs[start:end][cols].values\n",
    "            \n",
    "            pred = model.predict(X)\n",
    "            \n",
    "            ret.append(np.asarray(pred).astype(np.float16))\n",
    "            start += batch_size\n",
    "        pred = np.concatenate(ret)               \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    #print(pred.shape, type(pred))    \n",
    "        \n",
    "    return pred\n",
    "\n",
    "def create_submission (data, pairs, threshold ):\n",
    "    probs = {}\n",
    "\n",
    "    def get_probs(a,b):\n",
    "        if a == b:\n",
    "            return 1.0\n",
    "        if a < b:\n",
    "            return probs[(a,b)]\n",
    "        else:\n",
    "            return probs[(b,a)]\n",
    "\n",
    "    \n",
    "    print (\"create_submission\")\n",
    "    pairs = pairs.append ( pairs.rename(columns = {\"id\":\"match_id\", \"match_id\":\"id\"}) )\n",
    "    pairs = pairs.groupby([\"id\",\"match_id\"]).agg({\"pred\":\"mean\"}).reset_index()\n",
    "    pairs = pairs[pairs[\"pred\"] > threshold ]\n",
    "    \n",
    "    \n",
    "    print (\"load probs\")\n",
    "    \n",
    "    for a, b, c in tqdm(zip ( pairs[\"id\"].values, pairs[\"match_id\"].values, pairs[\"pred\"].values ), total=len(pairs)):\n",
    "        if a < b:\n",
    "            probs[(a,b)] = c\n",
    "        else:\n",
    "            probs[(b,a)] = c\n",
    "    \n",
    "    ids = data[\"id\"].unique()\n",
    "    id_df = pd.DataFrame ({\n",
    "        \"id\": ids,\n",
    "        \"match_id\":ids\n",
    "    })\n",
    "\n",
    "    \n",
    "    submission = pairs[['id', 'match_id']]\n",
    "    submission = submission.append ( id_df )\n",
    "\n",
    "    print (\"create list\")\n",
    "\n",
    "    submission = submission.groupby('id')['match_id'].apply(list).reset_index()\n",
    "    submission[\"match_id\"] = submission[\"match_id\"].map(lambda x: list(set(x)))\n",
    "    \n",
    "    sol = defaultdict(list)\n",
    "\n",
    "    mat = submission[[\"id\",\"match_id\"]].values\n",
    "    for k in tqdm(range ( mat.shape[0] )):\n",
    "        id = mat[k,0]\n",
    "        matches = mat[k,1]\n",
    "\n",
    "        #first level (a,b) -> (b,a)\n",
    "        for x in matches:\n",
    "            if not x in sol[id]:\n",
    "                sol[id].append(x)\n",
    "            if not id in sol[x]:\n",
    "                sol[x].append(id)\n",
    "\n",
    "        #second level (a,b), (a,c) -> (b,c) ?\n",
    "        matches.remove(id)\n",
    "        if len(matches) > 2:\n",
    "            for x, y in combinations(matches, 2):\n",
    "                    if y not in sol[x]:\n",
    "                        sol[x].append(y)\n",
    "                    if x not in sol[y]:\n",
    "                        sol[y].append(x)    \n",
    "\n",
    "    submission[\"match_id\"] = submission[\"id\"].map(sol)\n",
    "    submission['match_id'] = submission['match_id'].map(lambda x: list(set(x)))\n",
    "    submission['matches'] = submission['match_id'].apply(lambda x: ' '.join(set(x)))\n",
    "\n",
    "    return submission [[\"id\", \"matches\"]]\n",
    "\n",
    "\n",
    "\n",
    "XGB_MODELS = [\n",
    "    \n",
    "    \"../input/flm-xgb-99h-v06a-sample-50\",\n",
    "    \"../input/flm-xgb-99h-v06b-sample-50\",\n",
    "    \"../input/flm-xgb-99h-v06c-sample-50\",\n",
    "    \"../input/flm-xgb-99h-v06d-sample-50\",    \n",
    "    \n",
    "    \"../input/flm-xgb-99h-v06a-sample-65-s01\",\n",
    "    \"../input/flm-xgb-99h-v06b-sample-65-s01\",\n",
    "    \"../input/flm-xgb-99h-v06c-sample-65-s01\",\n",
    "    \"../input/flm-xgb-99h-v06d-sample-65-s01\",\n",
    "    \n",
    "    \"../input/flm-xgb-99h-v06a-sample-65-s02\",\n",
    "    \"../input/flm-xgb-99h-v06b-sample-65-s02\",\n",
    "    \"../input/flm-xgb-99h-v06c-sample-65-s02\",\n",
    "    \"../input/flm-xgb-99h-v06d-sample-65-s02\",\n",
    "\n",
    "    \n",
    "    \"../input/flm-xgb-99h-v06a-sample-65\",\n",
    "    \"../input/flm-xgb-99h-v06b-sample-65\",\n",
    "    \"../input/flm-xgb-99h-v06c-sample-65\",\n",
    "    \"../input/flm-xgb-99h-v06d-sample-65\",\n",
    "]\n",
    "\n",
    "\n",
    "LGB_MODELS = [\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 80_000\n",
    "THRESHOLD = 0.65\n",
    "\n",
    "SIZE = len(XGB_MODELS + LGB_MODELS)\n",
    "\n",
    "\n",
    "\n",
    "def main ():\n",
    "    print(\"read pairs\")\n",
    "    test_pairs = pd.read_feather(\"pairs.feather\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"drop id, match_id\")  \n",
    "\n",
    "    test_pairs = test_pairs.drop( ['id', 'match_id'], axis=1)\n",
    "    cols = [ c for c in  test_pairs.columns if c not in (\"label\", \"index\") ]\n",
    "\n",
    "\n",
    "    print(\"predict pairs\")\n",
    "    \n",
    "    \n",
    "    for n, model in enumerate (XGB_MODELS): \n",
    "        p = predict_xgb (f\"{model}/model.txt\", test_pairs, cols, batch_size = BATCH_SIZE)\n",
    "        if n == 0:\n",
    "            pred = p/SIZE\n",
    "        else:\n",
    "            pred += p/SIZE\n",
    "\n",
    "    for n, model in enumerate (LGB_MODELS): \n",
    "        p = predict_lgb (f\"{model}/model.txt\", test_pairs, cols, batch_size = BATCH_SIZE)\n",
    "        pred += p/SIZE\n",
    "\n",
    "    \n",
    "    del test_pairs\n",
    "    gc.collect()    \n",
    "    \n",
    "    \n",
    "    print(\"read pairs\")\n",
    "    test_pairs = pd.read_feather(\"pairs.feather\")\n",
    "    test_pairs = test_pairs [[\"id\", \"match_id\"]]\n",
    "    test_pairs[\"pred\"] = pred\n",
    "\n",
    "    print(\"read data\")\n",
    "    test = pd.read_feather(\"data.feather\")\n",
    "    \n",
    "    submission  = create_submission (test, test_pairs, THRESHOLD)\n",
    "    \n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6613587",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:03:16.932993Z",
     "iopub.status.busy": "2022-07-05T07:03:16.932339Z",
     "iopub.status.idle": "2022-07-05T07:04:20.566072Z",
     "shell.execute_reply": "2022-07-05T07:04:20.564954Z"
    },
    "papermill": {
     "duration": 63.653451,
     "end_time": "2022-07-05T07:04:20.568181",
     "exception": false,
     "start_time": "2022-07-05T07:03:16.914730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read pairs\r\n",
      "drop id, match_id\r\n",
      "predict pairs\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.04it/s]\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 357.11it/s]\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 337.43it/s]\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 301.21it/s]\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 29.64it/s]\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 356.69it/s]\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 346.04it/s]\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 361.36it/s]\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 175.03it/s]\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 269.97it/s]\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 353.26it/s]\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 281.61it/s]\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 348.80it/s]\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 353.71it/s]\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 348.74it/s]\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 359.32it/s]\r\n",
      "read pairs\r\n",
      "read data\r\n",
      "create_submission\r\n",
      "load probs\r\n",
      "0it [00:00, ?it/s]\r\n",
      "create list\r\n",
      "100%|█████████████████████████████████████| 100/100 [00:00<00:00, 455407.60it/s]\r\n"
     ]
    }
   ],
   "source": [
    "!python predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8cefb19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:04:20.605349Z",
     "iopub.status.busy": "2022-07-05T07:04:20.605018Z",
     "iopub.status.idle": "2022-07-05T07:04:21.271988Z",
     "shell.execute_reply": "2022-07-05T07:04:21.271005Z"
    },
    "papermill": {
     "duration": 0.687757,
     "end_time": "2022-07-05T07:04:21.273821",
     "exception": false,
     "start_time": "2022-07-05T07:04:20.586064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm *.npy *.vec *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18cb0cc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-05T07:04:21.310988Z",
     "iopub.status.busy": "2022-07-05T07:04:21.310322Z",
     "iopub.status.idle": "2022-07-05T07:04:21.334065Z",
     "shell.execute_reply": "2022-07-05T07:04:21.333146Z"
    },
    "papermill": {
     "duration": 0.044688,
     "end_time": "2022-07-05T07:04:21.336723",
     "exception": false,
     "start_time": "2022-07-05T07:04:21.292035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.46 ms, sys: 929 µs, total: 4.39 ms\n",
      "Wall time: 4.31 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E_000001272c6c5d</td>\n",
       "      <td>E_000001272c6c5d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E_000002eae2a589</td>\n",
       "      <td>E_000002eae2a589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E_000007f24ebc95</td>\n",
       "      <td>E_000007f24ebc95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E_000008a8ba4f48</td>\n",
       "      <td>E_000008a8ba4f48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E_00001d92066153</td>\n",
       "      <td>E_00001d92066153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>E_000610912e6c85</td>\n",
       "      <td>E_000610912e6c85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>E_00063a791601cc</td>\n",
       "      <td>E_00063a791601cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>E_000641a9bc89cf</td>\n",
       "      <td>E_000641a9bc89cf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>E_000643f43d5c02</td>\n",
       "      <td>E_000643f43d5c02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>E_00064faa02eea8</td>\n",
       "      <td>E_00064faa02eea8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id           matches\n",
       "0   E_000001272c6c5d  E_000001272c6c5d\n",
       "1   E_000002eae2a589  E_000002eae2a589\n",
       "2   E_000007f24ebc95  E_000007f24ebc95\n",
       "3   E_000008a8ba4f48  E_000008a8ba4f48\n",
       "4   E_00001d92066153  E_00001d92066153\n",
       "..               ...               ...\n",
       "95  E_000610912e6c85  E_000610912e6c85\n",
       "96  E_00063a791601cc  E_00063a791601cc\n",
       "97  E_000641a9bc89cf  E_000641a9bc89cf\n",
       "98  E_000643f43d5c02  E_000643f43d5c02\n",
       "99  E_00064faa02eea8  E_00064faa02eea8\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "submission = pd.read_csv(\"submission.csv\")\n",
    "\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 264.061194,
   "end_time": "2022-07-05T07:04:21.772646",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-05T06:59:57.711452",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
