{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data\n",
    "\n",
    "Load sales data from S3 / HDFS. We use the built-in \"csv\" method, which can use the first line has column names and which also supports infering the schema automatically. We use both and save some code for specifying the schema explictly.\n",
    "\n",
    "We also peek inside the data by retrieving the first five records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = spark.read\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .csv(\"s3://dimajix-training/data/kc-house-data\")\n",
    "\n",
    "data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Inspect Schema\n",
    "\n",
    "Now that we have loaded the data and that the schema was inferred automatically, let's inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the schema of raw_data\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Initial Investigations\n",
    "\n",
    "As a first step to get an idea of our data, we create some simple visualizations. We use the Python matplot lib package for creating simple two-dimensional plots, where the x axis will be one of the provided attributes and the y axis will be the house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import relevant Python packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 House Price in Relation to sqft_living\n",
    "\n",
    "Probably one of the most important attributes is the size of the house. This is provided in the data in the column \"sqft_living\". We extract the price column and the sqft_living column and create a simple scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract price and one of the attributes\n",
    "price = data.select(\"price\").toPandas()\n",
    "sqft_living = data.select(\"sqft_living\").toPandas()\n",
    "\n",
    "# Create simple scatter plot\n",
    "plt.plot(sqft_living, price, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 House Price in Relation to sqft_lot\n",
    "\n",
    "Another interesting attribute for predicting the house price might be the size of the whole lot, which is provided in the column \"sqft_lot\". So let's create another plot, now with \"price\" and \"sqft_lot\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "price = data.select(\"price\").toPandas()\n",
    "sqft_lot = data.select(\"sqft_lot\").toPandas()\n",
    "\n",
    "plt.plot(sqft_lot, price, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 House Price in Relation to bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "price = data.select(\"price\").toPandas()\n",
    "sqft_lot = data.select(\"bathrooms\").toPandas()\n",
    "\n",
    "plt.plot(sqft_lot, price, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Build Simple Linear Model\n",
    "\n",
    "Now since there seems to be some relation between the house price and some attributes, we try to fit a linear model to our data. This will be performed in multiple small steps. Later on we will see a more concise syntax for specifying all steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Extract features by using VectorAssembler\n",
    "\n",
    "Most Spark ML methods require one feature column of type `Vector`. In order to generate this feature column from the raw data, Spark provides a `VectorAssembler` which assembles one feature column from arbitrary source columns. The source columns have to be of type `double`.\n",
    "\n",
    "We use it to automatically extract the columns\n",
    "\n",
    "    bedrooms, bathrooms, sqft_living, sqft_lot, sqft_above, sqft_basement, sqft_living15, sqft_lot15\n",
    "\n",
    "into the new output column 'features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "\n",
    "cols = [\n",
    "        'bedrooms',\n",
    "        'bathrooms',\n",
    "        'sqft_living',\n",
    "        'sqft_lot',\n",
    "        'sqft_above',\n",
    "        'sqft_basement',\n",
    "        'sqft_living15',\n",
    "        'sqft_lot15'\n",
    "    ]\n",
    "\n",
    "# Build a VectorAssembler using the columns above\n",
    "tx = # YOUR CODE HERE\n",
    "\n",
    "# Transform the data\n",
    "td = # YOUR CODE HERE\n",
    "\n",
    "# Inspect the resulting schema\n",
    "td.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "td.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Split training / validation set\n",
    "\n",
    "First we need to split the data into a training and a validation set. Spark already provides a DataFrame method called `randomSplit` which takes an array of weights (between 0 and 1) and creates as many subsets. In our example, we want to create a training data set with 80% and the validation set should contain the remaining 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data - 80% for training, 20% for validation\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(\"training_data = \" + str(training_data.count()))\n",
    "print(\"validation_data = \" + str(validation_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Build Model\n",
    "\n",
    "Now that we have split up our data, we can fit a model to the training data. But before doing so, we again need to apply the `VectorAssembler` to the data to extract the features column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import *\n",
    "\n",
    "# Create a LinearRegession algorithm and configure it to match our data\n",
    "regression = # YOUR CODE HERE\n",
    "\n",
    "# Train a linear model using the regression above\n",
    "model = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Predict\n",
    "\n",
    "Make predictions from test data and print some results. We use the `validation_data` DataFrame (which was not used during training). Since this DataFrame does not already contain the feature column, we also need to apply the previously configured `VectorAssembler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create features using the VectorAssembler\n",
    "validation_features = # YOUR CODE HERE\n",
    "\n",
    "# Transform the resulting DataFrame using the trained model\n",
    "prediction = # YOUR CODE HERE\n",
    "\n",
    "# Print result\n",
    "prediction.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Evaluation\n",
    "\n",
    "Finally lets evaluate the prediction. The less the difference between the real value and the predicted value, the better our model performs. But of course we need a definition of what is *near*. PySpark already provides some simple built-in metrics for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import *\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Adding more Features\n",
    "\n",
    "The RMSE tells us that on average our prediction actually performs pretty bad. How can we improve that? Obviously we used only the size of the house for the price prediction so far, but we have a whole lot of additional information. So let's make use of that. The mathematical idea is that we create a more complex (but still linear) model that also includes other features.\n",
    "\n",
    "Let's recall that a linear  model looks as follows:\n",
    "\n",
    "    y = SUM(coeff[i]*x[i]) + intercept\n",
    "    \n",
    "This means that we are not limited to single feature `x`, but we can use many features `x[0]...x[n]`. Let's do that with the house data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data\n",
    "\n",
    "Since we don't have any additional information, we model some of the features differently. So far we used all features as direct linear predictors, which implies that a grade of 4 is twice as good as 2. Maybe that is not the case and not all predictors have a linear influence. Specifically nominal and ordinal features should be modeled differntly as categories. More an that later.\n",
    "\n",
    "First let's have a look at the data agin using Spark `describe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally let's check how many different zip codes are present in the data. If they are not too many, we could consider creating a one-hot encoded feature from the zip codes. We use the SQL function `countDistinct` to find the number of different zip codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count the number of distinct ZIP Codes using the SQL function countDistinct\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 New Features using One-Hot Encoding\n",
    "\n",
    "A simple but powerful method for creating new features from categories (i.e. nominal and ordinal features) is to use One-Hot-Encoding. For each nominal feature, the set of all possible values is indexed from 0 to some n. But since it cannot be assumed that larger values for n have a larger impact, a different approach is chosen. Instead each possible values is encoded by a 0/1 vector with only a single entry being one.\n",
    "\n",
    "Lets try that with the tools Spark provides to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Nominal Data\n",
    "First we need to index the data. Since Spark cannot know, which or how many distinct values are present in a specific column, the `StringIndexer` works like a ML algorithm: First it needs to be fit to the data, thereby returning an `StringIndexerModel` which then can be used for transforming data.\n",
    "\n",
    "Let's perform both steps and let us look at the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "\n",
    "indexer = StringIndexer() \\\n",
    "    .setInputCol(\"zipcode\") \\\n",
    "    .setOutputCol(\"zipcode_idx\") \\\n",
    "    .setHandleInvalid(\"keep\")\n",
    "\n",
    "# Create index model using the `fit` method\n",
    "index_model = # YOUR CODE HERE\n",
    "\n",
    "# Apply the index by using the `transform` method of the index model\n",
    "indexed_zip_data = # YOUR CODE HERE\n",
    "\n",
    "# Inspect the result\n",
    "indexed_zip_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also inspect the mapping by accessing all labels of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way of configuring the indexer is to specify all relevant parameters in its constructor as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexer = StringIndexer(\n",
    "    inputCol = \"zipcode\",\n",
    "    outputCol = \"zipcode_idx\",\n",
    "    handleInvalid = \"keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoder\n",
    "\n",
    "Now we have a single number (the index of the value) in a new column `zipcode_idx`. But in order to use the information in a linear model, we need to create sparse vectors from this index with only exactly one `1`. This can be done with the `OneHotEncoder` transformer. This time no fitting is required, the class can be used directly with its `transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder() \\\n",
    "    .setInputCol(\"zipcode_idx\") \\\n",
    "    .setOutputCol(\"zipcode_onehot\")\n",
    "\n",
    "encoded_zip_data = encoder.transform(indexed_zip_data)\n",
    "encoded_zip_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Creating Pipelines\n",
    "\n",
    "Since it would be tedious to add all features one after another and apply a full chain of transformations to the training set, the validation set and eventually to new data, Spark provides a `Pipeline` abstraction. A Pipeline simply contains a sequence of Transformations and (possibly multiple) machine learning algorithms. The whole pipeline then can be trained using the `fit` method which will return a `PipelineModel` instance. This instance contains all transformers and trained models and then can be used directly for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.regression import *\n",
    "\n",
    "pipeline = Pipeline(stages = [\n",
    "    # We use the SQLTransformer to calculate the absolute age and the age since the last renovation\n",
    "    # Store both ages in \"abs_age\" and \"renovated_age\"\n",
    "\n",
    "    # For every nominal feature, you have to create a pair of StringIndexer and OneHotEncoder. \n",
    "    # The StringIndexer should store its index result in some new column, which then is used \n",
    "    # by the OneHotEncoder to create a one-hot vector.\n",
    "    StringIndexer(\n",
    "        inputCol = \"bathrooms\",\n",
    "        outputCol = \"bathrooms_idx\",\n",
    "        handleInvalid = \"keep\"),\n",
    "    OneHotEncoder(\n",
    "        inputCol = \"bathrooms_idx\",\n",
    "        outputCol = \"bathrooms_onehot\"),\n",
    "    # Add StringIndexers and OneHotEncoders for the following nominal columns:\n",
    "    # \"bedrooms\", \"floors\", \"grade\", \"zipcode\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # In addition add OneHotEncoder for the columns \"view\" and \"condition\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Now add a VectorAssembler which collects all newly derived columns from above and the following numeric columns:\n",
    "    # \"sqft_living\", \"sqft_lot\", \"waterfront\", \"sqft_above\", \"sqft_basement\", \"yr_built\", \"yr_renovated\", \"sqft_living15\", \"sqft_lot15\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Finally add a LinearRegression which uses the output of the VectorAssembler as features and the\n",
    "    # target variable \"price\" as label column\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with training data\n",
    "\n",
    "Once you created the `Pipeline`, you can fit it in a single step using the `fit` method. This will return an instance of the class `PipelineModel`. Assign this model instace to a value called `model`.\n",
    "\n",
    "And remember: Use the training data for fitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Evaluate model using validation data\n",
    "\n",
    "Now that we have a model, we need to measure its performance. This requires that predictions are created by applying the model to the validation data by using the `transform` method of the moodel. The quality metric of the prediction is implemented in the `RegressionEvaluator` class from the Spark ML evaluation package. Create an instance of the evaluator and configure it appropriately to use the column `price` as the target (label) variable and the column `prediction` (which has been created by the pipeline model) as the prediction column. Also remember to set the metric name to `rmse`. Finally feed in the predicted data into the evaluator, which in turn will calculate the desired quality metric (RMSE in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import *\n",
    "\n",
    "# Create and configure a RegressionEvaluator\n",
    "evaluator = # YOUR CODE HERE\n",
    "\n",
    "# Create predictions of the validationData by using the \"transform\" method of the model\n",
    "pred = # YOUR CODE HERE\n",
    "\n",
    "# Now measure the quality of the prediction by using the \"evaluate\" method of the evaluator\n",
    "rmse = # YOUR CODE HERE\n",
    "\n",
    "print(\"RMSE = \" + str(rmse))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark 2.1 (Python 3.5)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
