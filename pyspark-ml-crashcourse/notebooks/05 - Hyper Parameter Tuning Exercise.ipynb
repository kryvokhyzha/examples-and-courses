{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Data\n",
    "\n",
    "First we load data from HDFS. It is stored as a trivial CSV file with three columns\n",
    "1. product name\n",
    "2. review text\n",
    "3. rating (1 - 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>name</td>\n",
       "      <td>review</td>\n",
       "      <td>rating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planetwise Flannel Wipes</td>\n",
       "      <td>These flannel wipes are OK, but in my opinion ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0                                               name   \n",
       "1                           Planetwise Flannel Wipes   \n",
       "2                              Planetwise Wipe Pouch   \n",
       "3                Annas Dream Full Quilt with 2 Shams   \n",
       "4  Stop Pacifier Sucking without tears with Thumb...   \n",
       "\n",
       "                                              review  rating  \n",
       "0                                             review  rating  \n",
       "1  These flannel wipes are OK, but in my opinion ...       3  \n",
       "2  it came early and was not disappointed. i love...       5  \n",
       "3  Very soft and comfortable and warmer than it l...       5  \n",
       "4  This is a product well worth the purchase.  I ...       5  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema =  StructType([\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('review',StringType(), True),\n",
    "    StructField('rating',StringType(), True),\n",
    "])\n",
    "\n",
    "raw_data = spark.read.schema(schema).csv(\"s3://dimajix-training/data/amazon_baby\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Clean and Cache Data\n",
    "\n",
    "We need to convert the \"rating\" columns to an integer - but this will obviously fail for the first record, as this one contains the CSV header. So we need to perform some cleanup after trying to convert the data.\n",
    "\n",
    "For helping distributing the workload, we repartition the DataFrame and also cache it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lamaze Peekaboo, I Love You</td>\n",
       "      <td>One of baby's first and favorite books, and it...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our Baby Girl Memory Book</td>\n",
       "      <td>Really happy with this purchase. I was looking...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Semanario (7) Little Girls 14k Gold Overlay Ba...</td>\n",
       "      <td>. I am pleased with product. I love the bangle...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neurosmith - Music Blocks with Mozart Music Ca...</td>\n",
       "      <td>It takes a youthful spirit of inquiry and fasc...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fisher Price Nesting Action Vehicles</td>\n",
       "      <td>This is a great toy.  The wheels really work a...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0                        Lamaze Peekaboo, I Love You   \n",
       "1                          Our Baby Girl Memory Book   \n",
       "2  Semanario (7) Little Girls 14k Gold Overlay Ba...   \n",
       "3  Neurosmith - Music Blocks with Mozart Music Ca...   \n",
       "4               Fisher Price Nesting Action Vehicles   \n",
       "\n",
       "                                              review  rating  \n",
       "0  One of baby's first and favorite books, and it...       4  \n",
       "1  Really happy with this purchase. I was looking...       5  \n",
       "2  . I am pleased with product. I love the bangle...       4  \n",
       "3  It takes a youthful spirit of inquiry and fasc...       5  \n",
       "4  This is a great toy.  The wheels really work a...       5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = raw_data.withColumn('rating',col('rating').cast(IntegerType())) \\\n",
    "    .filter(col('rating').isNotNull()) \\\n",
    "    .filter(col('review').isNotNull()) \\\n",
    "    .repartition(31) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Split Train Data / Test Data\n",
    "\n",
    "Now let's do the usual split of our data into a training data set and a validation data set. Let's use 80% of all reviews for training and 20% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 139461\n",
      "test_data: 34861\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = data.randomSplit([0.8,0.2], seed=1)\n",
    "\n",
    "print(\"train_data: %d\" % train_data.count())\n",
    "print(\"test_data: %d\" % test_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Custom Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Implement Transformer for Removing Punctuations\n",
    "\n",
    "We need a custom Transformer to build the pipeline. The transformer should remove all punctuations from a given column containing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    import string\n",
    "    for c in string.punctuation:\n",
    "        text = text.replace(c, ' ')\n",
    "    return text\n",
    "\n",
    "\n",
    "class PunctuationCleanupTransformer(Transformer):\n",
    "    def __init__(self, inputCol, outputCol):\n",
    "        \"\"\"\n",
    "        Constructor of PunctuationCleanupTransformer which takes two arguments:\n",
    "        inputCol - name of input column\n",
    "        outputCol - name of output column\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"\n",
    "        Protecetd _transform method which will be called by the public transform\n",
    "        method. You should not call this method directly.\n",
    "        \"\"\"\n",
    "        remove_punctuation_udf = udf(remove_punctuations, StringType())\n",
    "        return dataset.withColumn(self.outputCol, remove_punctuation_udf(self.inputCol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Implement Transformer for Stemming\n",
    "\n",
    "We need to stem words, and for doing so we use the Python NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem_word(words):\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(word) for word in words]\n",
    "\n",
    "\n",
    "class PorterStemmerTransformer(Transformer):\n",
    "    def __init__(self, inputCol, outputCol):\n",
    "        \"\"\"\n",
    "        Constructor of PorterStemmerTransformer which takes two arguments:\n",
    "        inputCol - name of input column\n",
    "        outputCol - name of output column\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"\n",
    "        Protecetd _transform method which will be called by the public transform\n",
    "        method. You should not call this method directly.\n",
    "        \"\"\"\n",
    "        stem_word_udf = udf(stem_word, ArrayType(StringType()))\n",
    "        return dataset.withColumn(self.outputCol, stem_word_udf(self.inputCol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create ML Pipeline\n",
    "\n",
    "Now we have all components for creating an initial ML Pipeline. Remember that we have been using the following components before\n",
    "\n",
    "* PunctuationCleanupTransformer - remove punctuations from reviews\n",
    "* Tokenizer - for splitting reviews into words\n",
    "* StopWordRemover - for removing stop words\n",
    "* PorterStemmerTransformer - for stemming words\n",
    "* NGram - for creating NGrams (we'll use two words per n-gram)\n",
    "* CountVectorizer - for creating bag-of-word features from the words\n",
    "* IDF - for creating TF-IDF features from the NGram counts\n",
    "* LogisticRegression - for creating the real model\n",
    "\n",
    "You also need to transform the incoming rating (1-5) to a sentiment (0 or 1) and you need to drop reviews with a rating of 3. This can be done using one ore more SQLTransformer instances. Inside the SQLTransformer instance you simply write SQL code and access the current DataFrame via `__THIS__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "\n",
    "stopWords = ['the','a','and','or', 'it', 'this', 'of', 'an', 'as', 'in', 'on', 'is', 'are', 'to', 'was', 'for', 'then', 'i']\n",
    "stopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "\n",
    "stages = [\n",
    "    PunctuationCleanupTransformer(inputCol='review', outputCol='clean_review'),\n",
    "    SQLTransformer(statement='SELECT *,CASE WHEN rating < 3 THEN 0.0 ELSE 1.0 END AS sentiment FROM __THIS__ WHERE rating <> 3'),\n",
    "    Tokenizer(inputCol='clean_review', outputCol='words'),\n",
    "    StopWordsRemover(inputCol='words', outputCol='vwords', stopWords=stopWords),\n",
    "    PorterStemmerTransformer(inputCol='vwords', outputCol='stems'),\n",
    "    NGram(inputCol='stems', outputCol='ngrams', n=3),\n",
    "    CountVectorizer(inputCol='ngrams', outputCol='tf', minDF=2.0),\n",
    "    IDF(inputCol='tf', outputCol='features'),\n",
    "    LogisticRegression(featuresCol='features',labelCol='sentiment')\n",
    "]\n",
    "pipe = Pipeline(stages = stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Model Evaluation\n",
    "As in the original exercise, we want to use a custom metric for assessing the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import *\n",
    "\n",
    "class AccuracyClassificationEvaluator(Evaluator):\n",
    "    def __init__(self, predictionCol='prediction', labelCol='label'):\n",
    "        super(Evaluator,self).__init__()\n",
    "        self.predictionCol = predictionCol\n",
    "        self.labelCol = labelCol\n",
    "    \n",
    "    def _evaluate(self, dataset):\n",
    "        num_total = dataset.count()\n",
    "        num_correct = dataset.filter(col(self.labelCol) == col(self.predictionCol)).count()\n",
    "        accuracy = float(num_correct) / num_total\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Hyper Parameter Tuning\n",
    "\n",
    "The whole pipeline has some parameters which have an influence on the result, i.e. the accuracy. For example the size of the n-grams will probably have a big impact and also the minDF parameter of the CountVecttorizer will probably have some impact. These settings are called \"hyper parameters\", because they are also model parameters, but not learnt directly during the training phase. But which parameters work best?\n",
    "\n",
    "We will use a CrossValidation to select the best set of hyperparameters.\n",
    "\n",
    "First let us have a look at the paremeters of some stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputCol: input column name. (current: stems)\n",
      "n: number of elements per n-gram (>=1) (default: 2, current: 3)\n",
      "outputCol: output column name. (default: NGram_4f6286e3942710af6a00__output, current: ngrams)\n"
     ]
    }
   ],
   "source": [
    "print(pipe.getStages()[5].explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Create ParamGrid\n",
    "\n",
    "Now we create a param grid that should be used for using different sets of parameters. We want to tweak two parameters again:\n",
    "\n",
    "* NGRam sizes should take values in [2,3,5]\n",
    "* CounterVectoriter minimum document frequency (minDF) should take values in [1,2,3,5])\n",
    "\n",
    "In order to create this grid, we first need to retrieve the corresponding stages from the pipeline, so we can access its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import *\n",
    "\n",
    "ngram = pipe.getStages()[5] # Get NGrame stage\n",
    "count = ... # Get CountVectorizer Stage\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(ngram.n, ...) \\\n",
    "    .addGrid(..., ...) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Perform Hyper Parameter tuning using CrossValidator\n",
    "\n",
    "Now we can wrap the previous pipeline inside a CrossValidator which trains the model over and over again for all entries in the ParameterGrid. The CrossValidator works as a wrapper of the regression algorithm and will return a pipeline model. In order to evaluate the goodness of a fit, the CrossValidator also needs an Evaluator - we'll use our AccuracyClassificationEvaluator again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create instance of AccuracyClassificationEvaluator with labelCol set to the real sentiment column\n",
    "evaluator = ... # YOUR CODE HERE\n",
    "\n",
    "# Create a CrossValidator instance\n",
    "validator = CrossValidator(estimator=pipe, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Let the CrossValidator fit the best pipeline model\n",
    "model = validator.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict sentiment for test data\n",
    "pred = model.transform(test_data)\n",
    "always_positive = pred.withColumn('prediction',lit(1.0))\n",
    "\n",
    "print(\"Model Accuracy = %f\" % evaluator.evaluate(pred))\n",
    "print(\"Baseline Accuracy = %f\" % evaluator.evaluate(always_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark 2.1 (Python 3.5)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
